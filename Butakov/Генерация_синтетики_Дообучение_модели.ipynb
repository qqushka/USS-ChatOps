{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HW9sCIUio_Y"
      },
      "source": [
        "# Генерация датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-07-25T05:42:33.329111Z",
          "iopub.status.busy": "2024-07-25T05:42:33.328447Z",
          "iopub.status.idle": "2024-07-25T05:44:06.216964Z",
          "shell.execute_reply": "2024-07-25T05:44:06.215979Z",
          "shell.execute_reply.started": "2024-07-25T05:42:33.329074Z"
        },
        "id": "gnTIR1Wsio_c",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "f4566f53-edbe-466c-b66e-25a254b8894e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install langchain pymupdf transformers torch langchain_community rank_bm25 ragatouille unstructured \"unstructured[pdf]\" update poppler-utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wygG1DUply2x"
      },
      "source": [
        "За основу для генерации датасета - мы берём чанки из нормативных документов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-25T05:44:22.819748Z",
          "iopub.status.busy": "2024-07-25T05:44:22.819384Z",
          "iopub.status.idle": "2024-07-25T05:44:22.825009Z",
          "shell.execute_reply": "2024-07-25T05:44:22.823495Z",
          "shell.execute_reply.started": "2024-07-25T05:44:22.819719Z"
        },
        "id": "079jKWXHio_e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter( #20\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"\n",
        "    ],\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-24T08:04:26.990165Z",
          "iopub.status.busy": "2024-07-24T08:04:26.989414Z",
          "iopub.status.idle": "2024-07-24T08:04:30.352522Z",
          "shell.execute_reply": "2024-07-24T08:04:30.351573Z",
          "shell.execute_reply.started": "2024-07-24T08:04:26.990132Z"
        },
        "id": "D2fJY_0hio_e",
        "outputId": "dc1ba243-7be6-46da-bd4f-b294a7f3b042",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:03<00:00, 10.38it/s]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
        "loader = DirectoryLoader(\"/kaggle/input/legall/Legal_doc_NEW\", glob=\"**/*.pdf\", show_progress=True, loader_cls=PyMuPDFLoader)#.load_and_split()\n",
        "documents = loader.load_and_split(text_splitter)\n",
        "#documents=loader\n",
        "texts = [doc.page_content for doc in documents]\n",
        "metadata = [doc.metadata for doc in documents]\n",
        "docs_df = pd.DataFrame({'text': texts, 'metadata': metadata})\n",
        "docs_df = docs_df[docs_df['text'].apply(len) >= 150]\n",
        "docs_df = docs_df.reset_index(drop=True)\n",
        "print(docs_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-07-25T10:21:38.936499Z",
          "iopub.status.busy": "2024-07-25T10:21:38.936029Z",
          "iopub.status.idle": "2024-07-25T10:21:53.404325Z",
          "shell.execute_reply": "2024-07-25T10:21:53.403183Z",
          "shell.execute_reply.started": "2024-07-25T10:21:38.936464Z"
        },
        "id": "-Tgmz3KHio_h",
        "outputId": "252466b5-0428-42e2-a3af-439bd3dc7e1c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pip install openai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_z0qp-LlVHU"
      },
      "source": [
        "Генерация синтетического датасета, используя gpt-4o-mini. Генерация происходит батчами при помощи batch api openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7d6abb8ee62740328a6ab0d087cffcbc",
            "d0107e22164645e8ad9b1ab53d82463a",
            "34fa0c48f3ea4b499d062e1b40cbc259"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-07-25T10:25:00.773184Z",
          "iopub.status.busy": "2024-07-25T10:25:00.772769Z",
          "iopub.status.idle": "2024-07-25T10:39:10.621609Z",
          "shell.execute_reply": "2024-07-25T10:39:10.619908Z",
          "shell.execute_reply.started": "2024-07-25T10:25:00.773152Z"
        },
        "id": "5IekwvQ6io_j",
        "outputId": "6a40fb57-b5f2-4a56-f560-e64dbd094b65",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d6abb8ee62740328a6ab0d087cffcbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0107e22164645e8ad9b1ab53d82463a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34fa0c48f3ea4b499d062e1b40cbc259",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '' #chatgpt key\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "client = openai.OpenAI()\n",
        "\n",
        "def generate_request_payload(text, custom_id, model=\"gpt-4o-mini\", request_type=\"question\"):\n",
        "    if request_type == \"question\":\n",
        "        message = f\"Прочитайте следующий текст и задайте к нему вопрос. В вопросах ты не должен ссылаться напрямую на текст. Задвай вопрос так, как будто у тебя нет контекста: {text[:500]}\"\n",
        "    elif request_type == \"negative_example\":\n",
        "        message = f\"Напиши одно предложение, которое будет являться противоположным относительно Пример: {text[:500]}. Оно должно иметь стиль,структуру текста, как в примере, но при этом не быть слишком похожим. Пиши сразу ответы без нумерации и чего-то ещё лишнего.\"\n",
        "\n",
        "    return {\n",
        "        \"custom_id\": custom_id,\n",
        "        \"method\": \"POST\",\n",
        "        \"url\": \"/v1/chat/completions\",\n",
        "        \"body\": {\n",
        "            \"model\": model,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": message}],\n",
        "            \"max_tokens\": 100 if request_type == \"question\" else 600,\n",
        "            \"temperature\": 0.3 if request_type == \"question\" else 0.2\n",
        "        }\n",
        "    }\n",
        "\n",
        "def create_batch_file(texts, filename=\"batch_input.jsonl\", request_type=\"question\"):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for i, text in enumerate(texts):\n",
        "            request_payload = generate_request_payload(text, custom_id=f\"request-{i+1}\", request_type=request_type)\n",
        "            f.write(json.dumps(request_payload, ensure_ascii=False) + '\\n')\n",
        "    return filename\n",
        "\n",
        "def upload_batch_file(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        batch_input_file = client.files.create(file=f, purpose=\"batch\")\n",
        "    return batch_input_file.id\n",
        "\n",
        "def create_batch(batch_input_file_id):\n",
        "    batch = client.batches.create(\n",
        "        input_file_id=batch_input_file_id,\n",
        "        endpoint=\"/v1/chat/completions\",\n",
        "        completion_window=\"24h\",\n",
        "        metadata={\"description\": \"synthetic data generation\"}\n",
        "    )\n",
        "    return batch.id\n",
        "\n",
        "def check_batch_status(batch_id, timeout=3600, interval=60):\n",
        "    start_time = time.time()\n",
        "    while True:\n",
        "        batch = client.batches.retrieve(batch_id)\n",
        "        status = batch.status\n",
        "        if status == 'completed':\n",
        "            return batch\n",
        "        elif status in ['failed', 'expired']:\n",
        "            raise Exception(f\"Batch failed or expired with status: {status}\")\n",
        "        elif time.time() - start_time > timeout:\n",
        "            raise TimeoutError(\"Batch status check timed out\")\n",
        "        time.sleep(interval)\n",
        "\n",
        "def retrieve_batch_results(output_file_id):\n",
        "    file_response = client.files.content(output_file_id)\n",
        "    return file_response.text\n",
        "\n",
        "def generate_synthetic_data_with_negatives(texts):\n",
        "    questions_batch_file = create_batch_file(texts, filename=\"batch_questions.jsonl\", request_type=\"question\")\n",
        "    questions_batch_file_id = upload_batch_file(questions_batch_file)\n",
        "    questions_batch_id = create_batch(questions_batch_file_id)\n",
        "\n",
        "    questions_batch = check_batch_status(questions_batch_id)\n",
        "    questions_results = retrieve_batch_results(questions_batch.output_file_id)\n",
        "    questions = {json.loads(result)['custom_id']: json.loads(result)['response']['body']['choices'][0]['message']['content'] for result in questions_results.splitlines()}\n",
        "\n",
        "    negatives_batch_file = create_batch_file(texts, filename=\"batch_negatives.jsonl\", request_type=\"negative_example\")\n",
        "    negatives_batch_file_id = upload_batch_file(negatives_batch_file)\n",
        "    negatives_batch_id = create_batch(negatives_batch_file_id)\n",
        "\n",
        "    negatives_batch = check_batch_status(negatives_batch_id)\n",
        "    negatives_results = retrieve_batch_results(negatives_batch.output_file_id)\n",
        "    negatives = {json.loads(result)['custom_id']: json.loads(result)['response']['body']['choices'][0]['message']['content'] for result in negatives_results.splitlines()}\n",
        "\n",
        "    examples = []\n",
        "    for i, text in enumerate(texts):\n",
        "        example = {\n",
        "            'queries': questions[f\"request-{i+1}\"],\n",
        "            'positive': text,\n",
        "            'negative': negatives[f\"request-{i+1}\"],\n",
        "        }\n",
        "        examples.append(example)\n",
        "\n",
        "    return examples\n",
        "\n",
        "texts = docs_df['text'].tolist()\n",
        "all_synthetic_data = generate_synthetic_data_with_negatives(texts)\n",
        "\n",
        "\n",
        "random.shuffle(all_synthetic_data)\n",
        "\n",
        "total_texts = len(docs_df)\n",
        "train_size = int(0.8 * total_texts)\n",
        "val_size = int(0.15 * total_texts)\n",
        "test_size = total_texts - train_size - val_size\n",
        "\n",
        "train_examples = all_synthetic_data[:train_size]\n",
        "val_examples = all_synthetic_data[train_size:train_size + val_size]\n",
        "test_examples = all_synthetic_data[train_size + val_size:train_size + val_size + test_size]\n",
        "\n",
        "\n",
        "with open('train_dataset.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(train_examples, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "with open('val_dataset.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(val_examples, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "with open('test_dataset.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(test_examples, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "train_dataset = Dataset.from_json('train_dataset.json')\n",
        "val_dataset = Dataset.from_json('val_dataset.json')\n",
        "test_dataset = Dataset.from_json('test_dataset.json')\n",
        "\n",
        "\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': val_dataset,\n",
        "    'test': test_dataset\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-07-25T11:43:01.399391Z",
          "iopub.status.busy": "2024-07-25T11:43:01.399041Z",
          "iopub.status.idle": "2024-07-25T11:43:01.461172Z",
          "shell.execute_reply": "2024-07-25T11:43:01.460004Z",
          "shell.execute_reply.started": "2024-07-25T11:43:01.399362Z"
        },
        "id": "juMHfTEMio_j",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "1e0c774e-c54a-4427-f01e-c86ddcd35841",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for i in test_dataset:\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRcQDy1qio_j"
      },
      "source": [
        "# Обучение эмбед модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-25T10:41:32.601116Z",
          "iopub.status.busy": "2024-07-25T10:41:32.600694Z",
          "iopub.status.idle": "2024-07-25T10:41:46.805175Z",
          "shell.execute_reply": "2024-07-25T10:41:46.803685Z",
          "shell.execute_reply.started": "2024-07-25T10:41:32.601071Z"
        },
        "id": "RdVv9pMNio_k",
        "outputId": "67ee905b-3545-4c03-9893-de3d6362800e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install datasets sentence-transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-25T11:43:01.462771Z",
          "iopub.status.busy": "2024-07-25T11:43:01.462460Z",
          "iopub.status.idle": "2024-07-25T11:43:01.497702Z",
          "shell.execute_reply": "2024-07-25T11:43:01.496754Z",
          "shell.execute_reply.started": "2024-07-25T11:43:01.462744Z"
        },
        "id": "V0Yq_3laio_k",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_json('/kaggle/working/train_dataset.json')\n",
        "val_dataset = Dataset.from_json('/kaggle/working/val_dataset.json')\n",
        "test_dataset = Dataset.from_json('/kaggle/working/test_dataset.json')\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.rename_column('queries', 'anchor')\n",
        "val_dataset = val_dataset.rename_column('queries', 'anchor')\n",
        "test_dataset = test_dataset.rename_column('queries', 'anchor')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-07-25T10:19:46.244055Z",
          "iopub.status.busy": "2024-07-25T10:19:46.243055Z",
          "iopub.status.idle": "2024-07-25T10:20:07.021515Z",
          "shell.execute_reply": "2024-07-25T10:20:07.019309Z",
          "shell.execute_reply.started": "2024-07-25T10:19:46.244019Z"
        },
        "id": "0kIIQC6Zio_l",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "ae4b50c0-cff0-4e1d-fba2-5fee65ffb9c3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for i in train_dataset:\n",
        "    print(\"Вопрос\", i['anchor'])\n",
        "    print(\"Позитив\", i['positive'])\n",
        "    print()\n",
        "    print(\"Негатив\", i['negative'])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpcrVbpVio_l",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from sentence_transformers import (\n",
        "    SentenceTransformer,\n",
        "    SentenceTransformerTrainer,\n",
        "    SentenceTransformerTrainingArguments,\n",
        "    SentenceTransformerModelCardData,\n",
        ")\n",
        "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
        "from sentence_transformers.training_args import BatchSamplers\n",
        "from sentence_transformers.evaluation import TripletEvaluator\n",
        "from sentence_transformers.training_args import BatchSamplers\n",
        "\n",
        "\n",
        "model = SentenceTransformer(\n",
        "    \"intfloat/multilingual-e5-small\"\n",
        ")\n",
        "\n",
        "train_dataset = train_dataset\n",
        "eval_dataset = val_dataset\n",
        "test_dataset = test_dataset\n",
        "\n",
        "loss = MultipleNegativesRankingLoss(model)\n",
        "\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"models/zxc\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_ratio=0.1,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=5,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=5,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=5,\n",
        "    #run_name=\"asd\",\n",
        ")\n",
        "\n",
        "dev_evaluator = TripletEvaluator(\n",
        "    anchors=eval_dataset[\"anchor\"],\n",
        "    positives=eval_dataset[\"positive\"],\n",
        "    negatives=eval_dataset[\"negative\"],\n",
        "    name=\"all-nli-dev\",\n",
        ")\n",
        "dev_evaluator(model)\n",
        "\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    loss=loss,\n",
        "    evaluator=dev_evaluator,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "test_evaluator = TripletEvaluator(\n",
        "    anchors=test_dataset[\"anchor\"],\n",
        "    positives=test_dataset[\"positive\"],\n",
        "    negatives=test_dataset[\"negative\"],\n",
        "    name=\"all-nli-test\",\n",
        ")\n",
        "test_evaluator(model)\n",
        "\n",
        "model.save_pretrained(\"e5-small-fine_tune\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA35tGy_io_l"
      },
      "source": [
        "# Проверка embed model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-07-25T09:56:14.860168Z",
          "iopub.status.busy": "2024-07-25T09:56:14.859840Z",
          "iopub.status.idle": "2024-07-25T09:58:17.446428Z",
          "shell.execute_reply": "2024-07-25T09:58:17.445210Z",
          "shell.execute_reply.started": "2024-07-25T09:56:14.860141Z"
        },
        "id": "W4YTnonuio_m",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "efd3763e-33a2-4780-ff1d-8708922f8a7d",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install langchain pymupdf transformers torch langchain_community ragatouille  faiss-gpu accelerate bitsandbytes unstructured \"unstructured[pdf]\" update poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-25T11:45:54.732914Z",
          "iopub.status.busy": "2024-07-25T11:45:54.732634Z",
          "iopub.status.idle": "2024-07-25T11:45:54.739717Z",
          "shell.execute_reply": "2024-07-25T11:45:54.738697Z",
          "shell.execute_reply.started": "2024-07-25T11:45:54.732888Z"
        },
        "id": "BLCzdkyeio_m",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import time\n",
        "from IPython.display import Markdown, display\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-07-25T11:45:54.741205Z",
          "iopub.status.busy": "2024-07-25T11:45:54.740864Z",
          "iopub.status.idle": "2024-07-25T11:45:56.006335Z",
          "shell.execute_reply": "2024-07-25T11:45:56.004917Z",
          "shell.execute_reply.started": "2024-07-25T11:45:54.741174Z"
        },
        "id": "O4FCfleuio_m",
        "outputId": "db70dcf6-3804-4064-ffd0-c18a258dddbd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Загрузка\n",
        "model_ckpt =\"/kaggle/working/e5-small-fine_tune\" #\"/kaggle/working/output/multilingual_bi_encoder-2024-07-24_02-50-45\"#\"/kaggle/working/mmmmmmmm\" #\"intfloat/multilingual-e5-small\" #\"/kaggle/working/output/train_bi-encoder-intfloat-multilingual-e5-small-2024-07-23_03-47-54\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModel.from_pretrained(model_ckpt)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-25T11:45:56.008040Z",
          "iopub.status.busy": "2024-07-25T11:45:56.007746Z",
          "iopub.status.idle": "2024-07-25T11:45:59.256535Z",
          "shell.execute_reply": "2024-07-25T11:45:59.253207Z",
          "shell.execute_reply.started": "2024-07-25T11:45:56.008013Z"
        },
        "id": "CUi5Ysdsio_n",
        "outputId": "53cafe5d-749a-45e8-d29d-0f8bb03b96e4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:02<00:00, 10.80it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter( #20\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"\n",
        "    ],\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "# Загрузка документов\n",
        "from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
        "loader = DirectoryLoader(\"/kaggle/input/legall/Legal_doc_NEW\", glob=\"**/*.pdf\", show_progress=True, loader_cls=PyMuPDFLoader)#.load_and_split()\n",
        "documents = loader.load_and_split(text_splitter)\n",
        "#documents=loader\n",
        "texts = [doc.page_content for doc in documents]\n",
        "metadata = [doc.metadata for doc in documents]\n",
        "docs_df = pd.DataFrame({'text': texts, 'metadata': metadata})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-25T11:45:59.258877Z",
          "iopub.status.busy": "2024-07-25T11:45:59.258469Z",
          "iopub.status.idle": "2024-07-25T11:46:27.507684Z",
          "shell.execute_reply": "2024-07-25T11:46:27.506661Z",
          "shell.execute_reply.started": "2024-07-25T11:45:59.258835Z"
        },
        "id": "KjNIAHj3io_n",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def cls_pooling(model_output):\n",
        "    return model_output.last_hidden_state[:, 0]\n",
        "\n",
        "def get_embeddings(text_list: List[str], batch_size: int = 0):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(text_list)):\n",
        "        batch = text_list[i]\n",
        "        encoded_input = tokenizer(\n",
        "            batch, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
        "        with torch.no_grad():\n",
        "            model_output = model(**encoded_input)\n",
        "        batch_embeddings = cls_pooling(model_output).detach().cpu().numpy()\n",
        "        embeddings.append(batch_embeddings)\n",
        "        torch.cuda.empty_cache()\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "\n",
        "embeddings = get_embeddings(texts, batch_size=0)\n",
        "#\n",
        "#import numpy as np\n",
        "#import pickle\n",
        "\n",
        "#def save_embeddings_to_drive(embeddings, file_path):\n",
        "#    with open(file_path, 'wb') as f:\n",
        "#        pickle.dump(embeddings, f)\n",
        "#\n",
        "\n",
        "#embeddings_file_path = '/content/drive/MyDrive/embeddings.npy'\n",
        "#save_embeddings_to_drive(embeddings, embeddings_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-23T04:09:35.773978Z",
          "iopub.status.busy": "2024-07-23T04:09:35.773094Z",
          "iopub.status.idle": "2024-07-23T04:09:35.784897Z",
          "shell.execute_reply": "2024-07-23T04:09:35.783829Z",
          "shell.execute_reply.started": "2024-07-23T04:09:35.773944Z"
        },
        "id": "5c_H5HeEio_n",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "embeddings_file_path = '/kaggle/working/embeddings.pkl'\n",
        "def load_embeddings_from_drive(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        embeddings = pickle.load(f)\n",
        "    return embeddings\n",
        "\n",
        "# Пример загрузки\n",
        "embeddings = load_embeddings_from_drive(embeddings_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-23T04:08:43.960421Z",
          "iopub.status.busy": "2024-07-23T04:08:43.959635Z",
          "iopub.status.idle": "2024-07-23T04:08:43.989495Z",
          "shell.execute_reply": "2024-07-23T04:08:43.988570Z",
          "shell.execute_reply.started": "2024-07-23T04:08:43.960391Z"
        },
        "id": "EX6bZgKVio_o",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#def save_embeddings_to_drive(embeddings, file_path):\n",
        "#    with open(file_path, 'wb') as f:\n",
        "#        pickle.dump(embeddings, f)\n",
        "\n",
        "\n",
        "#embeddings_file_path = '/kaggle/working/embeddings.pkl'\n",
        "#save_embeddings_to_drive(embeddings, embeddings_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-25T11:46:27.510238Z",
          "iopub.status.busy": "2024-07-25T11:46:27.509806Z",
          "iopub.status.idle": "2024-07-25T11:46:27.590786Z",
          "shell.execute_reply": "2024-07-25T11:46:27.589419Z",
          "shell.execute_reply.started": "2024-07-25T11:46:27.510187Z"
        },
        "id": "DdtJ_qjkio_o",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "faiss_index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "faiss_index.add(embeddings)\n",
        "\n",
        "faq_data = pd.read_excel('/kaggle/input/legall/filtered_updated_faq_data (2) (3).xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-25T11:46:27.594230Z",
          "iopub.status.busy": "2024-07-25T11:46:27.593874Z",
          "iopub.status.idle": "2024-07-25T11:46:27.602964Z",
          "shell.execute_reply": "2024-07-25T11:46:27.601978Z",
          "shell.execute_reply.started": "2024-07-25T11:46:27.594200Z"
        },
        "id": "g7PfSZ4nio_o",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_numbers(text):\n",
        "    numbers = re.findall(r'(\\d+)\\)', text)\n",
        "    filtered_numbers = [num for num in numbers if len(num) <= 2]\n",
        "    return ','.join(filtered_numbers)\n",
        "\n",
        "\n",
        "faq_data['PDF Title'] = faq_data['PDF Title'].apply(extract_numbers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-25T11:46:27.604640Z",
          "iopub.status.busy": "2024-07-25T11:46:27.604337Z",
          "iopub.status.idle": "2024-07-25T11:46:27.615846Z",
          "shell.execute_reply": "2024-07-25T11:46:27.614832Z",
          "shell.execute_reply.started": "2024-07-25T11:46:27.604614Z"
        },
        "id": "wneMVQnwio_o",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def clean_sets(sets):\n",
        "    cleaned_sets = []\n",
        "    for s in sets:\n",
        "        new_set = set()\n",
        "        for item in s:\n",
        "            if ',' in item:\n",
        "                new_set.update(filter(None, item.split(',')))\n",
        "            elif item != ',':\n",
        "                new_set.add(item)\n",
        "        cleaned_sets.append(new_set)\n",
        "    return cleaned_sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-25T11:46:27.618413Z",
          "iopub.status.busy": "2024-07-25T11:46:27.617998Z",
          "iopub.status.idle": "2024-07-25T11:46:28.899992Z",
          "shell.execute_reply": "2024-07-25T11:46:28.898513Z",
          "shell.execute_reply.started": "2024-07-25T11:46:27.618336Z"
        },
        "id": "yYA9WQ6Aio_o",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "qwe=[]\n",
        "def perform_combined_search(\n",
        "    question: str,\n",
        "    embeddings: List[List[float]],\n",
        "    faiss_index: faiss.Index,\n",
        "    num_retrieved_docs: int = 3,\n",
        "    rewrite_question_flag: bool = False,\n",
        ") -> Tuple[List[Tuple[str, float, str]], float]:\n",
        "    start_time = time.time()\n",
        "\n",
        "    qwe.append(question)\n",
        "    question_embedding = get_embeddings(['query:'+question])\n",
        "    D, I = faiss_index.search(question_embedding, num_retrieved_docs)\n",
        "    faiss_results = [(docs_df.iloc[i], D[0][j]) for j, i in enumerate(I[0])]\n",
        "\n",
        "    combined_results = {}\n",
        "    for doc, score in faiss_results:\n",
        "        file_path = doc['metadata']['file_path']\n",
        "        text_fragment = doc['text']\n",
        "        if file_path not in combined_results or score < combined_results[file_path][0]:\n",
        "            combined_results[file_path] = (score, text_fragment)\n",
        "\n",
        "    combined_results = combined_results.items()\n",
        "    retrieval_time = time.time() - start_time\n",
        "    return [(extract_numbers(res[0]), res[1][0], res[1][1]) for res in combined_results], retrieval_time\n",
        "\n",
        "results = []\n",
        "\n",
        "for _, row in faq_data.iterrows():\n",
        "    question = row['Question']\n",
        "    expected_file_paths = set(row['PDF Title'].split(','))\n",
        "\n",
        "    unique_file_paths_with_scores, retrieval_time = perform_combined_search(question, embeddings, faiss_index)\n",
        "    unique_file_paths = set([res[0] for res in unique_file_paths_with_scores])\n",
        "\n",
        "    cleaned_expected_file_paths = clean_sets([expected_file_paths])[0]\n",
        "    cleaned_unique_file_paths = clean_sets([unique_file_paths])[0]\n",
        "\n",
        "    true_positives = len(cleaned_unique_file_paths & cleaned_expected_file_paths)\n",
        "    false_positives = len(cleaned_unique_file_paths - cleaned_expected_file_paths)\n",
        "    false_negatives = len(cleaned_expected_file_paths - cleaned_unique_file_paths)\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives)\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    results.append({\n",
        "        'Question': question,\n",
        "        'Expected File Paths': cleaned_expected_file_paths,\n",
        "        'Predicted File Paths with Scores': unique_file_paths_with_scores,\n",
        "        'True Positives': true_positives,\n",
        "        'False Positives': false_positives,\n",
        "        'False Negatives': false_negatives,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Score': f1_score,\n",
        "        'Retrieval Time (s)': retrieval_time,\n",
        "        'Rerank Time (s)': 0\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "total_true_positives = results_df['True Positives'].sum()\n",
        "total_false_positives = results_df['False Positives'].sum()\n",
        "total_false_negatives = results_df['False Negatives'].sum()\n",
        "\n",
        "average_precision = results_df['Precision'].mean()\n",
        "average_recall = results_df['Recall'].mean()\n",
        "average_f1_score = results_df['F1 Score'].mean()\n",
        "\n",
        "print(\"-------------------------------------------------------------------\")\n",
        "print(f\"Total True Positives: {total_true_positives}\")\n",
        "print(f\"Total False Positives: {total_false_positives}\")\n",
        "print(f\"Average Precision: {average_precision:.2f}\")\n",
        "print(f\"Average Recall: {average_recall:.2f}\")\n",
        "print(f\"Average F1 Score: {average_f1_score:.2f}\")\n",
        "print(\"-------------------------------------------------------------------\")\n",
        "#\n",
        "#\n",
        "#\n",
        "#def display_markdown(text):\n",
        "#    display(Markdown(text))\n",
        "#\n",
        "#for index, row in results_df.iterrows():\n",
        "#    print(row['Question'])\n",
        "#    display_markdown(f\"# Вопрос: {row['Question']}\")\n",
        "#    #display_markdown(f\"Если есть llm: {qwe[index]}\")\n",
        "#    display_markdown(f\"## **Нужные PDF:**\")\n",
        "#    for path in row['Expected File Paths']:\n",
        "#        display_markdown(f\" - **{path}**\")\n",
        "#    print()\n",
        "#    display_markdown(\"## **Предсказанные PDF:**\")\n",
        "#    for path, score, text_fragment in row['Predicted File Paths with Scores']:\n",
        "#        display_markdown(f\"  - **{path} (Растояние: {score:.2f})**\")\n",
        "#        #display_markdown(f\"**Фрагмент текста:**\")\n",
        "#        print(f\"{text_fragment}\")\n",
        "#        print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 5423621,
          "sourceId": 9003050,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5431411,
          "sourceId": 9014292,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5426708,
          "sourceId": 9014429,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5432270,
          "sourceId": 9015486,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5437319,
          "sourceId": 9022687,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5438536,
          "sourceId": 9024269,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5423594,
          "sourceId": 9029575,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5442548,
          "sourceId": 9030017,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5442863,
          "sourceId": 9030452,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30746,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
