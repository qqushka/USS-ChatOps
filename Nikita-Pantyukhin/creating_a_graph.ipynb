{"cells":[{"cell_type":"code","execution_count":null,"id":"8bcaa156","metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1721115323021,"user":{"displayName":"qqushka","userId":"02057807309569807044"},"user_tz":-180},"id":"8bcaa156"},"outputs":[],"source":["import os  # Для работы с директориями и переменными в окружении\n","import re  # Для работы с регулярными выражениями"]},{"cell_type":"markdown","id":"c86bd367","metadata":{"id":"c86bd367"},"source":["# 1 - Парсинг текста из PDF"]},{"cell_type":"code","execution_count":null,"id":"7b1c4e6f","metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1721115337175,"user":{"displayName":"qqushka","userId":"02057807309569807044"},"user_tz":-180},"id":"7b1c4e6f"},"outputs":[],"source":["# Путь к PDF документов\n","folder_path_to_pdf = r\"path\\to\\NPA\""]},{"cell_type":"markdown","id":"9aed4c48","metadata":{"id":"9aed4c48"},"source":["## 1.1 - Использование PyMuPDF (fitz) для получения текста"]},{"cell_type":"code","execution_count":null,"id":"01561e36","metadata":{"executionInfo":{"elapsed":13231,"status":"ok","timestamp":1721115337170,"user":{"displayName":"qqushka","userId":"02057807309569807044"},"user_tz":-180},"id":"01561e36"},"outputs":[],"source":["import fitz  # Для чтения текста из PDF"]},{"cell_type":"code","execution_count":null,"id":"008ff2f9","metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1721115363191,"user":{"displayName":"qqushka","userId":"02057807309569807044"},"user_tz":-180},"id":"008ff2f9"},"outputs":[],"source":["def extract_text_from_pdf(pdf_path):\n","    text = \"\"\n","    try:\n","        with fitz.open(pdf_path) as doc:\n","            for page_num in range(len(doc)):\n","                page = doc.load_page(page_num)\n","                text += page.get_text()\n","    except Exception as e:\n","        print(f\"Ошибка извлечения текста из {pdf_path}: {e}\")\n","\n","    return text"]},{"cell_type":"markdown","id":"bab282b9","metadata":{"id":"bab282b9"},"source":["## 1.2 - Определение оглавления и основного текста документа"]},{"cell_type":"code","execution_count":null,"id":"fdb58c3e","metadata":{},"outputs":[],"source":["def separate_text_from_toc(text):\n","    # Паттерн для разделения документа\n","    match = re.compile(r'(.*?)(?:(?:У(?:ТВЕРЖДЕН|твержден)(?:(?:А|а)|(?:Ы|ы)))\\s+приказом\\s+ФСТЭК\\s+России|'\n","                       r'Зарегистрировано\\s+в\\s+Министерстве\\s+юстиции\\s+Российской\\s+Федерации|'\n","                       r'Одобрен\\s+Советом\\s+Федерации|Директор|Правительств(?:а|о))(.*)', re.DOTALL).search(text)\n","    if match:\n","        table_of_contents = match.group(1).strip()\n","        main_text = match.group(2).strip()\n","    else:\n","        table_of_contents = \"\"\n","        main_text = text.strip()\n","\n","    return table_of_contents, main_text"]},{"cell_type":"markdown","id":"2e23cb20","metadata":{},"source":["## 1.3 - Очистка текста от лишней и повторяющейся информации"]},{"cell_type":"code","execution_count":null,"id":"8f67c712","metadata":{},"outputs":[],"source":["# Словарь для преобразования русских месяцев в числовые\n","months = {\n","    'января': '01',\n","    'февраля': '02',\n","    'марта': '03',\n","    'апреля': '04',\n","    'мая': '05',\n","    'июня': '06',\n","    'июля': '07',\n","    'августа': '08',\n","    'сентября': '09',\n","    'октября': '10',\n","    'ноября': '11',\n","    'декабря': '12'\n","}\n","\n","def convert_russian_date(russian_date):\n","    day, month, year, _ = russian_date.split()\n","    return f'{day.zfill(2)}.{months[month]}.{year[:-2]}'\n","\n","def clean_text(text, doc_type, doc_date, doc_number):\n","    # Компиляция регулярного выражения для удаления идентификации документа от КонсультантПлюс\n","    pattern = re.compile(\n","        rf'{doc_type[:1].upper() + doc_type[1:].lower()} .*?{convert_russian_date(doc_date)}.*?N\\s*{doc_number}\\s*\\(ред\\.\\s*от\\s*\\d{{2}}\\.\\d{{2}}\\.\\d{{4}}\\)\\s*?\".*?(?:\\.\\.\\.|\")',\n","        re.DOTALL\n","    )\n","\n","    # Применение регулярного выражения\n","    text = re.sub(pattern, '', text).strip()\n","\n","    # Удаление ненужных фраз и информации от АО \"Кодекс\"\n","    text = re.split(\n","        r'Электронный\\s+текст\\s+документа\\s+подготовлен\\s+АО\\s+\"Кодекс\"\\s+и\\s+сверен\\s+по:',\n","        text,\n","        flags=re.IGNORECASE\n","    )[0]\n","\n","    # Удаление ненужных фраз и информации от КонсультантПлюс\n","    text = re.sub(\n","        r'(Документ предоставлен|www\\.consultant\\.ru|КонсультантПлюс|надежная\\s+правовая\\s+поддержка|Страница\\s*\\d+\\s*из\\s*\\d+\\s|Дата сохранения:\\s+\\d{2}\\.\\d{2}\\.\\d{4})',\n","        '',\n","        text,\n","        flags=re.IGNORECASE\n","    ).strip()\n","\n","    # Разделение текста на строки и удаление пустых строк\n","    lines_text = [line.strip() for line in text.split('\\n') if line.strip()]\n","\n","    # Удаление подчеркиваний и дефисов\n","    lines_text = [re.sub(r'(_+|-+)', '', line) for line in lines_text]\n","\n","    # Объединение очищенных строк в одну строку\n","    cleaned_text = ' '.join(lines_text)\n","    \n","    return cleaned_text"]},{"cell_type":"markdown","id":"15A_tP52wAvJ","metadata":{"id":"15A_tP52wAvJ"},"source":["## 1.4 - Извлечение идентификационных данных документа\n","- Оранжевое - организация;\n","- Фиолетовое - тип документа;\n","- Голубое - дата и номер документа.\n","\n","<img src=\"figures/document_identification.jpg\"/>"]},{"cell_type":"code","execution_count":null,"id":"r_q-1-bzwAAp","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1721130160942,"user":{"displayName":"qqushka","userId":"02057807309569807044"},"user_tz":-180},"id":"r_q-1-bzwAAp","outputId":"3954c1d7-6563-4c85-e9de-3a7d5031192f"},"outputs":[],"source":["def document_identification(text):\n","    # Удаляем строки, содержащие \"Зарегистрировано в Минюсте России\"\n","    text = re.sub(r'Зарегистрировано\\s+в\\s+Минюсте\\s+России\\s+\\d{1,2}\\s+\\w+\\s+\\d{4}\\s+г\\.\\s+N\\s+\\d+', '', text)\n","    \n","    # Паттерны для извлечения информации о документе\n","    document_pattern = re.compile(\n","        r\"(?:(\\d{1,2}\\s+\\w+\\s+\\d{4}\\s+г(?:\\.|ода)?)\\s+N\\s+(\\d+).*(ФЗ)?|)\\s*\"\n","        r\"(ФЕДЕРАЛЬНАЯ\\s+СЛУЖБА\\s+(?:БЕЗОПАСНОСТИ\\s+РОССИЙСКОЙ\\s+ФЕДЕРАЦИИ|ПО\\s+ТЕХНИЧЕСКОМУ\\s+И\\s+ЭКСПОРТНОМУ\\s+КОНТРОЛЮ)|\"\n","        r\"ПРАВИТЕЛЬСТВО\\s+РОССИЙСКОЙ\\s+ФЕДЕРАЦИИ|РОССИЙСКАЯ\\s+ФЕДЕРАЦИЯ|ПРЕЗИДЕНТ\\s+РОССИЙСКОЙ\\s+ФЕДЕРАЦИИ)\\s*\"\n","        r\"((?:ПРИКАЗ|ПОСТАНОВЛЕНИЕ|УКАЗ|ФЕДЕРАЛЬНЫЙ\\s+ЗАКОН))(?:\\s+от\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4}\\s+г(?:\\.|ода)?)\\s+(?:N|№)\\s+(\\d+)|)\"\n","    )\n","\n","    # Примеры:\n","    #   ФЕДЕРАЛЬНАЯ СЛУЖБА БЕЗОПАСНОСТИ РОССИЙСКОЙ ФЕДЕРАЦИИ ПРИКАЗ от 6 мая 2019 года N 196 Об утверждении Требований к средс...\n","    #   ПРАВИТЕЛЬСТВО РОССИЙСКОЙ ФЕДЕРАЦИИ ПОСТАНОВЛЕНИЕ от 8 февраля 2018 г. № 127 МОСКВА Об утверждении Правил категорирован...\n","    #   26 июля 2017 года N 187ФЗ РОССИЙСКАЯ ФЕДЕРАЦИЯ ФЕДЕРАЛЬНЫЙ ЗАКОН О БЕЗОПАСНОСТИ КРИТИЧЕСКОЙ ИНФОРМАЦИОННОЙ ИНФРАСТРУКТ...\n","    #   ПРЕЗИДЕНТ РОССИЙСКОЙ ФЕДЕРАЦИИ УКАЗ от 30 марта 2022 г. N 166 О МЕРАХ ПО ОБЕСПЕЧЕНИЮ ТЕХНОЛОГИЧЕСКОЙ НЕЗАВИСИМОСТИ И Б...\n","\n","    # Ищем соответствия в тексте\n","    document_match = document_pattern.search(text)\n","\n","    if document_match:\n","        if document_match.group(1) is None:\n","            organization = document_match.group(4)\n","            document_type = document_match.group(5)\n","            date = document_match.group(6)\n","            document_number = document_match.group(7)\n","        else:\n","            organization = document_match.group(4)\n","            document_type = document_match.group(5)\n","            date = document_match.group(1)\n","            document_number = document_match.group(2)\n","\n","        return {\n","            'organization': organization,\n","            'document_type': document_type,\n","            'date': date,\n","            'document_number': document_number\n","        }\n","    else:\n","        print(\"Не удалось извлечь информацию из текста\")\n","        return None"]},{"cell_type":"markdown","id":"4dz1wxsR6uQn","metadata":{"id":"4dz1wxsR6uQn"},"source":["## 1.5 - Извлечение заголовков и (под)пунктов\n","Используется счетчик для корректировки записи распознанного массива (под)пунктов."]},{"cell_type":"code","execution_count":null,"id":"e_f1YSS06tRE","metadata":{"executionInfo":{"elapsed":375,"status":"ok","timestamp":1721128875634,"user":{"displayName":"qqushka","userId":"02057807309569807044"},"user_tz":-180},"id":"e_f1YSS06tRE"},"outputs":[],"source":["def extract_titles_and_texts(text):\n","    \"\"\"\n","    Извлекает заголовки разделов с римскими цифрами, \"Статья X\" и заголовки по шаблону \"Приложение N X\".\n","    \"\"\"\n","    roman_pattern = re.compile(r\"(?<=\\b)([IVXLCDM]+\\.\\s+.+?)(?=\\s+[IVXLCDM]+\\.\\s|\\s*\\d+\\.\\s|$)\", re.MULTILINE | re.DOTALL)\n","    article_pattern = re.compile(r\"(?<=\\b)(Статья\\s+\\d+\\.\\s+.+?)(?=\\s+Статья\\s+\\d+\\.\\s|\\s*\\d+(?:\\.|\\))\\s|$)\", re.MULTILINE | re.DOTALL)\n","    appendix_order_pattern = re.compile(r\"(?<=\\b)(Приложение\\s+N\\s+\\d+\\s+к\\s+приказу\\s+ФСБ\\s+России\\s+от\\s+\\d+\\s+\\w+\\s+\\d{4}\\s+г(?:\\.|ода)\\s+N\\s+\\d+)(?=\\s+)\", re.MULTILINE | re.DOTALL)\n","\n","    matches = list(re.finditer(roman_pattern, text)) + list(re.finditer(article_pattern, text)) + list(re.finditer(appendix_order_pattern, text))\n","    matches.sort(key=lambda x: x.start())\n","\n","    results = []\n","    for i, match in enumerate(matches):\n","        title = match.group(1).strip()\n","        start_pos = match.end()\n","\n","        end_pos = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n","        results.append((title, text[start_pos:end_pos].strip()))\n","\n","    return results\n","\n","def get_item_numbers(text):\n","    \"\"\"\n","    Извлекает номера пунктов из текста.\n","    \"\"\"\n","    pattern = re.compile(r\"((?:\\d*\\.)*)\\s*\")\n","    match = pattern.match(text)\n","    if match:\n","        return match.group(1).strip().rstrip('.').split('.')\n","    \n","    return []\n","\n","def extract_subpoints(text, counter_status=['0'], register_counter_status=0):\n","    \"\"\"\n","    Извлекает подпункты из текста и проверяет их счетчиком.\n","    \"\"\"\n","    \n","    def update_counters(subpoint_number, counter_status, register_counter_status):\n","        \"\"\"\n","        Обновляет счетчики для подуровней разделов.\n","        \"\"\"\n","        if len(subpoint_number) > len(counter_status):\n","            register_counter_status += 1\n","            counter_status.append('0')\n","        elif len(subpoint_number) < len(counter_status):\n","            register_counter_status -= 1\n","            counter_status.pop()\n","\n","        counter_status[register_counter_status] = str(int(counter_status[register_counter_status]) + 1)\n","\n","        return counter_status, register_counter_status\n","\n","    subpoint_pattern = re.compile(r\"((?:\\d+\\.)+\\s*.+?)(?=(?:\\d+\\.)+|$)\", re.DOTALL)\n","    matches = re.findall(subpoint_pattern, text)\n","    subpoints = []\n","\n","    first_register_occupied = False\n","    \n","    for match in matches:\n","        subpoint = match.strip()\n","        subpoint_number = get_item_numbers(subpoint)\n","        if subpoint_number:\n","            counter_status, register_counter_status = update_counters(subpoint_number, counter_status, register_counter_status)\n","\n","            # Логирование\n","            # print(f\"Счетчик: {counter_status}, Распознано: {subpoint_number}, Уровень подпункта: {register_counter_status}\")\n","\n","            if not first_register_occupied:\n","                if subpoint_number == counter_status:\n","                    first_register_occupied = True\n","                else:\n","                    counter_status[register_counter_status] = str(int(counter_status[register_counter_status]) - 1)\n","            if first_register_occupied:\n","                if subpoint_number != counter_status:\n","                    if subpoints:\n","                        subpoints[-1] += ' ' + subpoint\n","                        counter_status[register_counter_status] = str(int(counter_status[register_counter_status]) - 1)\n","                else:\n","                    subpoints.append(subpoint)\n","        \n","    return subpoints, counter_status, register_counter_status"]},{"cell_type":"markdown","id":"faf21fca","metadata":{"id":"faf21fca"},"source":["## 1.6 - Получение информации из PDF"]},{"cell_type":"code","execution_count":null,"id":"6b7ce7b9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":420,"status":"ok","timestamp":1721130181365,"user":{"displayName":"qqushka","userId":"02057807309569807044"},"user_tz":-180},"id":"6b7ce7b9","outputId":"cf1b98c8-90bd-4e21-8f5a-ee1cdfa12ec5"},"outputs":[],"source":["def process_pdf_file(text, doc_info):\n","    \"\"\"\n","    Обрабатывает PDF файл: извлекает текст, разделяет его на оглавление и основной текст,\n","    идентифицирует документ, извлекает заголовки и их подпункты.\n","    \"\"\"\n","    titles_and_subpoints = {}\n","\n","    # Поиск заголовков\n","    titles = extract_titles_and_texts(text)\n","\n","    # Инициализация счетчиков подпунктов\n","    counter_subpoints = ['0']\n","    register_counter_status = 0\n","\n","    len_text_render = 50\n","    if titles:\n","        for title, content in titles:\n","            if re.match(r\"Статья\\s+\\d+\\.\", title) or re.match(r\"Приложение\\s+N\\s+\\d+\", title):\n","                # Обнуляем счетчики\n","                counter_subpoints = ['0']\n","                register_counter_status = 0\n","\n","                # Извлекаем нужный текст заголовка\n","                if re.match(r\"Приложение\\s+N\\s+\\d+\", title):\n","                    title = re.match(r\"Приложение\\s+N\\s+\\d+\", title).group()\n","            \n","            print(f\"\\t({len(title)}) {title}\")\n","            subpoints, counter_subpoints, register_counter_status = extract_subpoints(content, counter_subpoints, register_counter_status)\n","            titles_and_subpoints[title] = subpoints\n","            for subpoint in subpoints:\n","                print(f\"\\t\\t({len(subpoint)}) {subpoint[:len_text_render]} [..]\")\n","    else:\n","        subpoints, counter_subpoints, register_counter_status = extract_subpoints(text, counter_subpoints, register_counter_status)\n","        for subpoint in subpoints:\n","            print(f\"\\t({len(subpoint)}) {subpoint[:len_text_render]} [..]\")\n","        titles_and_subpoints[\"Без заголовка\"] = subpoints\n","\n","    return {\n","        'doc_info': doc_info,\n","        'titles_and_subpoints': titles_and_subpoints,\n","        'text': text\n","    }\n","\n","def extract_info_from_pdfs(folder_path):\n","    \"\"\"\n","    Извлекает информацию из всех PDF файлов в указанной папке.\n","    \"\"\"\n","    pdf_texts = {}\n","    for filename in os.listdir(folder_path):\n","        # Фильтр для вывода хорошо предобработанных докуменатов:\n","        # if filename.endswith(\".pdf\") and ('FSB' in filename or 'FSTEK' in filename) and 'FSTEK239' not in filename and 'FSTEK75' not in filename\n","        if filename.endswith(\".pdf\") and ('FSB' in filename or 'FSTEK' in filename) and 'FSTEK239' not in filename and 'FSTEK75' not in filename:\n","            pdf_file = os.path.join(folder_path, filename)\n","            print(filename)\n","            text = extract_text_from_pdf(pdf_file)\n","            if text:\n","                doc_info, text = separate_text_from_toc(text)\n","                doc_info = ' '.join([line.strip() for line in doc_info.split('\\n') if line.strip()])\n","                print(doc_info)\n","                doc_info = document_identification(doc_info)\n","                print(f\"Орг: {doc_info['organization']}\\nТип: {doc_info['document_type']}\\nДата: {doc_info['date']}\\nНомер: {doc_info['document_number']}\\n\")\n","\n","                text = clean_text(text, doc_info['document_type'], doc_info['date'], doc_info['document_number'])\n","                if text:\n","                    pdf_data = process_pdf_file(text, doc_info)\n","                    pdf_texts[filename] = {\n","                        'filename': filename,\n","                        **pdf_data\n","                    }\n","                    print('-' * 100)\n","            \n","    return pdf_texts\n","\n","pdf_texts = extract_info_from_pdfs(folder_path_to_pdf)"]},{"cell_type":"markdown","id":"5fe34bae","metadata":{"id":"5fe34bae"},"source":["# 2 - Подготовка, создание графа"]},{"cell_type":"markdown","id":"4446fc3a","metadata":{},"source":["## 2.1 - Подготовка"]},{"cell_type":"markdown","id":"e15645de","metadata":{},"source":["### 2.1.1 - Разделение текста на предложения\n","Для разделения большого текста на предложения используется функция `sent_tokenize` и набор пунктуации из библиотеки `nltk`.\n","\n","Если весь текст это предложение, то он будет разбит на чанки по знакам пунктуации."]},{"cell_type":"code","execution_count":null,"id":"604e54b0","metadata":{},"outputs":[],"source":["import nltk\n","from nltk.tokenize import sent_tokenize"]},{"cell_type":"code","execution_count":null,"id":"6f159631","metadata":{},"outputs":[],"source":["# Набор пунктуации nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"id":"5fe6e52a","metadata":{},"outputs":[],"source":["def split_text_by_punctuation(text, max_chunk_size):\n","    \"\"\"\n","    Разделяет текст примерно в середине, с учетом размера чанка и пунктуации.\n","    \"\"\"\n","    def find_split_index(text, max_chunk_size):\n","        # Найти индекс для разрыва текста около середины, но не превышая max_chunk_size\n","        mid = len(text) // 2\n","        # Найти ближайшую пунктуацию к середине текста\n","        split_points = re.finditer(r'[.,:;!?]\\s*', text)\n","        best_split = None\n","        for point in split_points:\n","            if point.start() <= mid <= point.end():\n","                best_split = point.end()\n","                break\n","            if point.start() < mid:\n","                best_split = point.end()\n","            else:\n","                break\n","            \n","        return best_split if best_split and best_split <= max_chunk_size else mid\n","\n","    chunks = []\n","    while len(text) > max_chunk_size:\n","        split_index = find_split_index(text, max_chunk_size)\n","        chunks.append(text[:split_index].strip())\n","        text = text[split_index:].strip()\n","    \n","    chunks.append(text)\n","\n","    return chunks"]},{"cell_type":"markdown","id":"e24c7c8b","metadata":{},"source":["### 2.1.2 - Суммаризация (Модель IlyaGusev/mbart_ru_sum_gazeta)\n","Если длина предложения превысит размер чанка, то из текста предложения достаётся смысловая выжимка."]},{"cell_type":"markdown","id":"29aae50b","metadata":{},"source":["Пример использования:\n"," - Пусть `article_text` это текст для обработки.\n","\n","`input_ids = tokenizer([article_text], max_length=600, truncation=True, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")`\n","\n","`output_ids = model.generate(input_ids=input_ids, no_repeat_ngram_size=4)[0]`\n","\n","`summary = tokenizer.decode(output_ids, skip_special_tokens=True)`"]},{"cell_type":"code","execution_count":null,"id":"e8f20fd5","metadata":{},"outputs":[],"source":["# Для инициализации модели для суммаризации текста\n","from transformers import MBartTokenizer, MBartForConditionalGeneration"]},{"cell_type":"code","execution_count":null,"id":"f9bf5c53","metadata":{},"outputs":[],"source":["summary_model_name = \"IlyaGusev/mbart_ru_sum_gazeta\"\n","\n","summary_tokenizer = MBartTokenizer.from_pretrained(summary_model_name)  # Скачивание модели\n","summary_model = MBartForConditionalGeneration.from_pretrained(summary_model_name)  # Загрузка модели\n","summary_model.to(\"cuda\")"]},{"cell_type":"code","execution_count":null,"id":"b9e4cbab","metadata":{},"outputs":[],"source":["def summary_text(text, chunk_size, summary_tokenizer, summary_model):\n","    \"\"\"\n","    Сжимает текст до размера чанка, используя модель суммаризации.\n","    \"\"\"\n","    len_summary = chunk_size + 1\n","    previous_len = len(text)\n","    no_change_count = 0\n","\n","    while len_summary > chunk_size and no_change_count < 3:\n","        input_ids = summary_tokenizer([text], max_length=chunk_size, truncation=True, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n","        output_ids = summary_model.generate(input_ids=input_ids, no_repeat_ngram_size=4)[0]\n","        new_text = summary_tokenizer.decode(output_ids, skip_special_tokens=True)\n","        \n","        if len(new_text) >= previous_len:\n","            no_change_count += 1\n","        else:\n","            no_change_count = 0\n","        \n","        text = new_text\n","        len_summary = len(text)\n","        previous_len = len(text)\n","    \n","    return text"]},{"cell_type":"markdown","id":"86dc3702","metadata":{},"source":["### 2.1.3 - Доступ к LLM через API от HuggingFace (mistralai/Mixtral-8x7B-Instruct-v0.1)\n","Инициализация LLMs для дальнейшего использования этого в функции, которая конвертирует текст в узлы и отношения между ними.\n","\n","Нужен [API ключ](https://huggingface.co/settings/tokens) в режиме `WRITE`.\n","\n","Выбранные модели:\n","1) mistralai/Mistral-7B-Instruct-v0.1\n","2) mistralai/Mistral-7B-Instruct-v0.2\n","3) mistralai/Mistral-7B-Instruct-v0.3\n","4) mistralai/Mixtral-8x7B-Instruct-v0.1\n","5) meta-llama/Meta-Llama-3-8B-Instruct"]},{"cell_type":"code","execution_count":null,"id":"6393053f","metadata":{},"outputs":[],"source":["from getpass import getpass  # Для хранения секретных ключей\n","\n","# Для инициализации и работы с LLM\n","from langchain_community.llms import HuggingFaceEndpoint\n","from langchain_experimental.graph_transformers import LLMGraphTransformer"]},{"cell_type":"code","execution_count":null,"id":"d5f26e32","metadata":{},"outputs":[],"source":["os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass(prompt=\"Введите API ключ от HuggingFaceHub\")"]},{"cell_type":"code","execution_count":null,"id":"84f36199","metadata":{},"outputs":[],"source":["models = [\n","    \"mistralai/Mistral-7B-Instruct-v0.1\",\n","    \"mistralai/Mistral-7B-Instruct-v0.2\",\n","    \"mistralai/Mistral-7B-Instruct-v0.3\",\n","    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n","    \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","]\n","\n","def initialize_transformer(model_id):\n","    hg_llm = HuggingFaceEndpoint(repo_id=model_id)\n","    \n","    return LLMGraphTransformer(llm=hg_llm)"]},{"cell_type":"markdown","id":"d2b7a578","metadata":{},"source":["### 2.1.4 - Работа с Neo4j в AuraDB\n","Граф собирается при использовании Neo4j через облачную версию [AuraDB](https://neo4j.com/cloud/platform/aura-graph-database/)."]},{"cell_type":"code","execution_count":null,"id":"c3ac0ce5","metadata":{},"outputs":[],"source":["from neo4j import GraphDatabase  # Для подключения к среде Neo4j"]},{"cell_type":"code","execution_count":null,"id":"63b0f1e4","metadata":{},"outputs":[],"source":["# Данные для входа в среду Neo4j\n","os.environ[\"NEO4J_URI\"] = \"neo4j+s://...\"\n","os.environ[\"NEO4J_USERNAME\"] = \"...\"\n","os.environ[\"NEO4J_PASSWORD\"] = \"...\""]},{"cell_type":"code","execution_count":null,"id":"c815c012","metadata":{},"outputs":[],"source":["class Neo4jGraph:\n","    def __init__(self):\n","        self.driver = GraphDatabase.driver(\n","            os.getenv(\"NEO4J_URI\"),\n","            auth=(os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n","        )\n","\n","    def close(self):\n","        \"\"\"\n","        Завершает работу с Neo4j.\n","        \"\"\"\n","        self.driver.close()\n","\n","    def create_node(self, node_text, node_type, properties={}):\n","        \"\"\"\n","        Добавляет узел, независимо от существующих в среде.\n","        Если узел с похожими параметрами уже есть, то создается клон с другим <id>.\n","        \"\"\"\n","        with self.driver.session() as session:\n","            result = session.run(f\"\"\"\n","                CREATE (n:{node_type} {{text: $node_text}})\n","                SET n += $properties\n","                RETURN elementId(n) AS node_id\n","            \"\"\", node_text=node_text, properties=properties)\n","            return result.single()[\"node_id\"]\n","\n","    def addition_node(self, node_text, node_type, properties={}):\n","        \"\"\"\n","        Добавляет узел, в зависимости от существующих в среде.\n","        Если узел с похожими параметрами уже есть, то функция обновит их на новые.\n","        \"\"\"\n","        with self.driver.session() as session:\n","            result = session.run(f\"\"\"\n","                MERGE (n:{node_type} {{text: $node_text}})\n","                SET n += $properties\n","                RETURN elementId(n) AS node_id\n","            \"\"\", node_text=node_text, properties=properties)\n","            return result.single()[\"node_id\"]\n","\n","    def add_relationship(self, source_node_id, target_node_id, rel_type):\n","        \"\"\"\n","        Добавляет отношения между узлами.\n","        \"\"\"\n","        with self.driver.session() as session:\n","            session.run(f\"\"\"\n","                MATCH (a) WHERE elementId(a) = $source_node_id\n","                MATCH (b) WHERE elementId(b) = $target_node_id\n","                MERGE (a)-[r:{rel_type}]->(b)\n","            \"\"\", source_node_id=source_node_id, target_node_id=target_node_id)\n","\n","    def find_nodes_by_text(self, node_text):\n","        \"\"\"\n","        Поиск узлов по полностью указанному тексту в них.\n","        \"\"\"\n","        with self.driver.session() as session:\n","            result = session.run(f\"\"\"\n","                MATCH (n {{text: $node_text}})\n","                RETURN n\n","            \"\"\", node_text=node_text)\n","            return [record[\"n\"] for record in result]\n","\n","    def find_similar_nodes_by_text(self, node_text):\n","        \"\"\"\n","        Поиск узлов по частично указанному тексту в них.\n","        \"\"\"\n","        def split_and_trim(text):\n","            \"\"\"\n","            Функция для разбивки текста на части и удаления половины символов у каждой части.\n","            \"\"\"\n","            parts = text.split()\n","            trimmed_parts = [part[:len(part)//2] for part in parts]\n","            return trimmed_parts\n","\n","        # Разбиваем текст и удаляем половину символов у каждой части\n","        trimmed_texts = split_and_trim(node_text)\n","\n","        # Формируем запрос\n","        query = \"MATCH (n) WHERE \"\n","        params = {}\n","        # Создаем условия для поиска узлов по каждому обрезанному тексту\n","        for i, trimmed_text in enumerate(trimmed_texts):\n","            query += f\"n.text CONTAINS $node_text{i}\"\n","            if i < len(trimmed_texts) - 1:\n","                query += \" AND \"\n","            params[f\"node_text{i}\"] = trimmed_text\n","        query += \" RETURN elementId(n) AS node_id\"\n","\n","        # Отправляем запрос\n","        with self.driver.session() as session:\n","            result = session.run(query, **params)\n","            return [record[\"node_id\"] for record in result]\n","\n","    def find_node_id_by_text(self, node_text):\n","        \"\"\"\n","        Поиск ID узла по тексту.\n","        \"\"\"\n","        with self.driver.session() as session:\n","            result = session.run(f\"\"\"\n","                MATCH (n {{text: $node_text}})\n","                RETURN elementId(n) AS node_id\n","            \"\"\", node_text=node_text)\n","            node = result.single()\n","            return node[\"node_id\"] if node else None\n","    \n","    def find_types_node_by_id(self, node_id):\n","        \"\"\"\n","        Получение состояния поля \"типы\" узла по ID. \n","        \"\"\"\n","        with self.driver.session() as session:\n","            result = session.run(f\"\"\"\n","                MATCH (n) WHERE elementId(n) = $node_id\n","                RETURN n.типы AS node_types\n","            \"\"\", node_id=node_id)\n","            record = result.single()\n","            return record[\"node_types\"] if record else None\n","    \n","    def updating_types_node_by_id(self, node_id, properties_types):\n","        \"\"\"\n","        Обновление состояния поля \"типы\" узла по ID.\n","        \"\"\"\n","        with self.driver.session() as session:\n","            session.run(f\"\"\"\n","                MATCH (n) WHERE elementId(n) = $node_id\n","                SET n.типы = $properties_types\n","            \"\"\", node_id=node_id, properties_types=properties_types)\n","    \n","    def find_text_node_by_id(self, node_id):\n","        \"\"\"\n","        Получение состояния поля \"text\" узла по ID. \n","        \"\"\"\n","        with self.driver.session() as session:\n","            result = session.run(f\"\"\"\n","                MATCH (n) WHERE elementId(n) = $node_id\n","                RETURN n.text AS node_text\n","            \"\"\", node_id=node_id)\n","            record = result.single()\n","            return record[\"node_text\"] if record else None\n","    \n","    def check_current_node_type_by_id(self, node_id):\n","        \"\"\"\n","        Получение типа узла по ID. \n","        \"\"\"\n","        with self.driver.session() as session:\n","            result = session.run(f\"\"\"\n","                MATCH (n) WHERE elementId(n) = $node_id\n","                RETURN labels(n) AS node_type\n","            \"\"\", node_id=node_id)\n","            record = result.single()\n","            return record[\"node_type\"] if record else None"]},{"cell_type":"markdown","id":"af9776b6","metadata":{},"source":["### 2.1.5 - Перевод текста на русский"]},{"cell_type":"code","execution_count":null,"id":"bacedeca","metadata":{},"outputs":[],"source":["from deep_translator import GoogleTranslator  # Для перевода текста на русский"]},{"cell_type":"code","execution_count":null,"id":"402116e5","metadata":{},"outputs":[],"source":["def translate_to_rus(text):\n","    # Проверяем, содержит ли текст английские буквы\n","    if re.search(r'[a-zA-Z]', text):\n","        return GoogleTranslator(source='en', target='ru').translate(text)\n","    else:\n","        return text"]},{"cell_type":"markdown","id":"025f4205","metadata":{},"source":["### 2.1.6 - Подготовка ID и типа узла перед добавлением "]},{"cell_type":"code","execution_count":null,"id":"bb61db14","metadata":{},"outputs":[],"source":["def prepare_text_node(text):\n","    \"\"\"\n","    Подготовка текста для вставки в узел.\n","\n","    Пример return: \"Тест текст\"\n","    \"\"\"\n","    \n","    # Проверка, является ли текст заголовком\n","    if re.match(r'^[IVXLCDM]+\\.\\s+', text, re.IGNORECASE) or \\\n","       re.match(r\"Статья\\s+\\d+\\.\", text, re.IGNORECASE) or \\\n","       re.match(r\"Приложение\\s+N\\s+\\d+\", text, re.IGNORECASE):\n","        return text\n","\n","    # Превращение возможных разделяющих знаков в пробелы\n","    text = re.sub(r'[-_]', ' ', text)\n","    # Удаление лишних пробелов и проверка наличия текста\n","    text = text.strip()\n","    if not text:\n","        return None\n","    # Вставка пробелов между маленькими и большими буквами\n","    text = re.sub(r'([a-zа-яё])([A-ZА-ЯЁ])', r'\\1 \\2', text).strip()\n","    # Попытка перевести текст\n","    text = translate_to_rus(text)\n","    # Форматирование аббревиатуры\n","    text = re.sub(r'(?:Gos SOPKA|Гос СОПКА|Госсопка|Гос сопка)', 'ГосСОПКА', text)\n","\n","    # Разделение текста на слова\n","    words = text.split(' ')\n","    formatted_words = []\n","    \n","    # Обработка первого слова\n","    if words:\n","        if re.search(r'[А-ЯЁA-Z]{2,}', words[0]):\n","            first_word = words[0]\n","        else:\n","            first_word = words[0].capitalize()\n","        formatted_words.append(first_word)\n","    \n","    # Обработка оставшихся слов\n","    for word in words[1:]:\n","        if re.search(r'[А-ЯЁA-Z]{2,}', word):\n","            formatted_words.append(word)\n","        else:\n","            formatted_words.append(word.lower())\n","\n","    return ' '.join(formatted_words)\n","\n","def prepare_type_node(text):\n","    \"\"\"\n","    Подготовка названия типа узла.\n","\n","    Пример return: \"ТестТекст\"\n","    \"\"\"\n","\n","    # Проверка на принадлежность к собственному набору\n","    if re.match(r'(?:Организация|НомерДокумента|Заголовок|Пункт)', text):\n","        return text\n","\n","    # Превращение возможных разделяющих знаков в пробелы\n","    text = re.sub(r'[-_]', ' ', text)\n","    # Удаление лишних пробелов и проверка наличия текста\n","    text = text.strip()\n","    if not text:\n","        return None\n","    # Вставка пробелов между маленькими и большими буквами\n","    text = re.sub(r'([a-zа-яё])([A-ZА-ЯЁ])', r'\\1 \\2', text).strip()\n","    # Попытка перевести текст\n","    text = translate_to_rus(text).replace('-', ' ')\n","    # Приведение к общему виду\n","    text = text.capitalize()\n","\n","    # Разделение текста на слова, приведение каждого слова к нижнему регистру, кроме первого символа\n","    text = ''.join([word.capitalize() for word in text.split()])\n","\n","    return text"]},{"cell_type":"markdown","id":"afbe3bbb","metadata":{},"source":["### 2.1.7 - Лемматизация слов для поиска похожих узлов"]},{"cell_type":"code","execution_count":null,"id":"81ca89ad","metadata":{},"outputs":[],"source":["from pymorphy3 import MorphAnalyzer"]},{"cell_type":"code","execution_count":null,"id":"3e8ac961","metadata":{},"outputs":[],"source":["# Инициализация морфологического анализатора\n","morph = MorphAnalyzer()\n","\n","def lemmatize_word(word):\n","    # Приведение слова к начальной форме\n","    p = morph.parse(word)[0]\n","    return p.normal_form\n","\n","def lemmatize_text(text):\n","    # Разделение текста на слова и лемматизация каждого слова\n","    words = text.split()\n","    lemmatized_words = [lemmatize_word(word) for word in words]\n","    lemmatized_words = ' '.join(lemmatized_words)\n","    return lemmatized_words[:1].upper() + lemmatized_words[1:]"]},{"cell_type":"markdown","id":"b9d1228d","metadata":{},"source":["## 2.2 - Создание графа"]},{"cell_type":"code","execution_count":null,"id":"6b4d8f16","metadata":{},"outputs":[],"source":["from langchain_core.documents import Document  # Для конвертации текста в допустимый формат функции \"convert_to_graph_documents\"\n","import json_repair  # Для работы функции \"convert_to_graph_documents\"\n","\n","import json  # Для логирования успешно завершенных этапов"]},{"cell_type":"code","execution_count":null,"id":"WxXy5GVrcAg6","metadata":{"id":"WxXy5GVrcAg6"},"outputs":[],"source":["chunk_size = 500"]},{"cell_type":"code","execution_count":null,"id":"e0ab8f50","metadata":{},"outputs":[],"source":["# Инициализируем граф\n","graph = Neo4jGraph()\n","\n","# Загружаем существующий лог или создаём новый\n","log_file = 'processed_files_log.json'\n","if os.path.exists(log_file):\n","    with open(log_file, 'r', encoding='utf-8') as file:\n","        processed_log = json.load(file)\n","else:\n","    processed_log = {}\n","\n","# Функция для обновления лога\n","def update_log(log_file, filename, title, paragraph, processed_log):\n","    if filename not in processed_log:\n","        processed_log[filename] = {}\n","    if title not in processed_log[filename]:\n","        processed_log[filename][title] = []\n","    processed_log[filename][title].append(paragraph)\n","    with open(log_file, 'w', encoding='utf-8') as file:\n","        json.dump(processed_log, file, ensure_ascii=False, indent=4)\n","\n","\n","# Функция для заполнения графа\n","def filling_in_graph(document, paragraph_id_node, graph, models):\n","    global hg_llm_transformer\n","    all_models_failed = True\n","    \n","    for model_id in models:\n","        try:\n","            # Инициализация LLM\n","            hg_llm_transformer = initialize_transformer(model_id)\n","            # Использование LLM для построения графа\n","            graph_documents = hg_llm_transformer.convert_to_graph_documents(document)\n","            # Вывод результатов\n","            print(f\"\\n({len(graph_documents[0].nodes)}) Nodes: {graph_documents[0].nodes}\")\n","            print(f\"({len(graph_documents[0].relationships)}) Relationships: {graph_documents[0].relationships}\")\n","            \n","            # Событие когда LLM выводит пустой граф\n","            if len(graph_documents[0].nodes) == 0:\n","                print(f\"\\n{model_id} выдала пустой граф.\\n\")\n","                continue\n","            \n","            all_models_failed = False\n","\n","            # Создание словаря для сохранения ID по тексту в нём\n","            individual_dictionary_nodes_ids = {}\n","            \n","            # Заполнение графа в Neo4j\n","            for graph_document in graph_documents:\n","                # Заполнение в Neo4j распознанных сущностей\n","                for node in graph_document.nodes:\n","                    # Подготовка данных\n","                    prepare_text = prepare_text_node(node.id)\n","                    prepare_type = prepare_type_node(node.type)\n","                    \n","                    # Исключение добавления None сущностей\n","                    if prepare_text == 'None' or prepare_type == 'None':\n","                        continue\n","                    \n","                    # Проверка на существование такого узла\n","                    find_node_id = graph.find_node_id_by_text(prepare_text)\n","                    individual_dictionary_nodes_ids[prepare_text] = None\n","                    if find_node_id == None:\n","                        # Попытка найти узел с похожим текстом\n","                        similar_nodes_ids = graph.find_similar_nodes_by_text(prepare_text)\n","                        if similar_nodes_ids != []:\n","                            print(f\"Обнаружено возможное совпадение в узлах: `{similar_nodes_ids}`\")\n","                            for similar_node_id in similar_nodes_ids:\n","                                # Проверка на одинаковое содержание\n","                                print(f\"Лемма текущего текста: `{lemmatize_text(prepare_text)}`; Лемма найденного текста: `{lemmatize_text(graph.find_text_node_by_id(similar_node_id))}`\")\n","                                if lemmatize_text(prepare_text) == lemmatize_text(graph.find_text_node_by_id(similar_node_id)):\n","                                    # Если найден узел с очень похожим содержанием, то\n","                                    # создаём отношение: \"Пункт\" -> \"X-старая-сущность\"\n","                                    print(f\"Обнаружен похожий текст: `{prepare_text}`; В узле c ID: `{similar_node_id}`\")\n","                                    graph.add_relationship(source_node_id=paragraph_id_node,\n","                                                   target_node_id=similar_node_id,\n","                                                   rel_type='СОДЕРЖАНИЕ')\n","                                    # Сохранение ID узла\n","                                    individual_dictionary_nodes_ids[prepare_text] = similar_node_id\n","                                    break # Остановка поиска\n","\n","                    # Если такой узел существует то обновляем в уже существующем узле поле \"типы\"    \n","                    else:\n","                        print(f\"Обнаружен одинаковый текст `{prepare_text}`; В узле c ID: `{find_node_id}`\")\n","                        # Поиск уже существующих типов узла\n","                        find_types = graph.find_types_node_by_id(find_node_id)\n","                        if find_types:\n","                            find_types = find_types.split('; ')\n","                        else:\n","                            find_types = []\n","                        # Добавление нового типа\n","                        # Исключение добавления присвоенного типа\n","                        current_node_type = graph.check_current_node_type_by_id(find_node_id)[0]\n","                        if prepare_type not in find_types and current_node_type != prepare_type:\n","                            find_types.append(prepare_type)\n","                            find_types = '; '.join(find_types)\n","                            graph.updating_types_node_by_id(node_id=find_node_id, properties_types=find_types)\n","                        # Сохранение ID узла\n","                        individual_dictionary_nodes_ids[prepare_text] = id_node\n","                    \n","                    # Если такого узла не нашлось, то он создаётся\n","                    if individual_dictionary_nodes_ids[prepare_text] == None:\n","                        # Создание отношения: \"Пункт\" -> \"X-новая-сущность\"\n","                        id_node = graph.addition_node(node_text=prepare_text, node_type=prepare_type)\n","                        print(f\"Создан новый узел c ID: `{id_node}`; Текст: `{prepare_text}`\")\n","                        graph.add_relationship(source_node_id=paragraph_id_node,\n","                                               target_node_id=id_node,\n","                                               rel_type='СОДЕРЖАНИЕ')\n","                        # Сохранение ID узла\n","                        individual_dictionary_nodes_ids[prepare_text] = id_node\n","                \n","                # Заполнение в Neo4j распознанных отношений между сущностями\n","                for relationship in graph_document.relationships:\n","                    # Подготовка текста для поиска ID узлов\n","                    prepare_source_node_text = prepare_text_node(relationship.source.id)\n","                    prepare_target_node_text = prepare_text_node(relationship.target.id)\n","\n","                    # Поиск ID узлов\n","                    relationship_source_id = 'None'\n","                    relationship_target_id = 'None'\n","                    # Поиск в словаре\n","                    if prepare_source_node_text in individual_dictionary_nodes_ids \\\n","                       and prepare_target_node_text in individual_dictionary_nodes_ids:\n","                        relationship_source_id = individual_dictionary_nodes_ids[prepare_source_node_text]\n","                        relationship_target_id = individual_dictionary_nodes_ids[prepare_target_node_text]\n","                    \n","                    print(f\"{relationship_source_id} -> {relationship_target_id}\")\n","                    if relationship_source_id == 'None' \\\n","                       or relationship_target_id == 'None':\n","                        print(f\"Словарь: {individual_dictionary_nodes_ids}\")\n","                        print(f\"Источник: `{prepare_source_node_text}`; Цель: `{prepare_target_node_text}`\")\n","\n","                    # Определение названия отношения\n","                    relationship_type = translate_to_rus(relationship.type.replace('_', ' ').replace('-', ' ')).upper().replace(' ', '_').replace('-', '_')\n","\n","                    # Исключение добавления None отношений и отношений на один и тот же узел\n","                    if relationship_source_id != 'None' \\\n","                       and relationship_target_id != 'None' \\\n","                       and relationship_source_id != relationship_target_id:\n","\n","                        graph.add_relationship(source_node_id=relationship_source_id,\n","                                               target_node_id=relationship_target_id,\n","                                               rel_type=relationship_type)\n","            \n","            print('-' * 100)\n","            # break  # Выход из цикла при первом результате\n","        except Exception as e:\n","            print(f\"\\nОшибка при использовании модели {model_id}: {e}\\n\")\n","            continue\n","    \n","    if all_models_failed:\n","        print(\"\\nВсе модели выдали плохой результат. Пропуск текста.\")\n","        print('-' * 100)\n","        return False\n","    return True\n","\n","\n","# Основной цикл обработки файлов\n","for filename in pdf_texts:\n","    document_info = pdf_texts[filename]['doc_info']\n","\n","    # Пропуск не идентифицированных документов\n","    if document_info is None:\n","        continue\n","    \n","    # Идентификация документа\n","    prepare_org_name = prepare_text_node(document_info['organization'])\n","    prepare_doc_number = prepare_text_node(document_info['document_number'])\n","    doc_type = document_info['document_type']\n","    doc_date = document_info['date']\n","\n","    # Создание отношения: \"Организация\" -> \"Документ\"\n","    org_id_node = graph.addition_node(node_text=prepare_org_name, node_type=prepare_type_node(\"Организация\"))\n","    doc_info_id_node = graph.addition_node(node_text=prepare_doc_number, node_type=prepare_type_node(\"НомерДокумента\"), properties={\"тип\": doc_type,\"дата\": doc_date})\n","    graph.add_relationship(source_node_id=org_id_node,\n","                           target_node_id=doc_info_id_node,\n","                           rel_type=\"ИЗДАННО\")\n","\n","    # Привязывание распознаваемых сущностей к заголовкам и пунктам\n","    for titles_and_subpoints in pdf_texts[filename]['titles_and_subpoints']:\n","\n","        # Если документ имеет хотя бы один заголовок, то создаётся отношение:\n","        #   \"Документ\" -> \"Заголовок\"\n","        if titles_and_subpoints != 'Без заголовка':\n","            prepare_title = prepare_text_node(titles_and_subpoints)\n","            title_id_node = graph.create_node(node_text=prepare_title, node_type=prepare_type_node('Заголовок'))\n","            graph.add_relationship(source_node_id=doc_info_id_node,\n","                                   target_node_id=title_id_node,\n","                                   rel_type='ЗАГОЛОВОК')\n","\n","        # Обработка (под)пунктов\n","        for text in pdf_texts[filename]['titles_and_subpoints'][titles_and_subpoints]:\n","            \n","            # Если документ имеет хотя бы один заголовок, то создаётся отношение:\n","            #   \"Заголовок\" -> \"Пункт\"\n","            prepare_paragraph = prepare_text_node(f\"{'.'.join(get_item_numbers(text))}.\")\n","            # if filename in processed_log and prepare_title in processed_log[filename] and prepare_paragraph in processed_log[filename][prepare_title]:\n","            #     print(f\"Пропуск обработанного пункта: `{prepare_paragraph}` в `{prepare_title}`\")\n","            #     continue\n","            paragraph_id_node = graph.create_node(node_text=prepare_paragraph, node_type=prepare_type_node('Пункт'))\n","            if titles_and_subpoints != 'Без заголовка':\n","                graph.add_relationship(source_node_id=title_id_node,\n","                                       target_node_id=paragraph_id_node,\n","                                       rel_type='СОДЕРЖАНИЕ')\n","            \n","            # Иначе узел \"Заголовок\" в цепочке пропускается. Остаётся:\n","            #   \"Документ\" -> \"Пункт\"\n","            else:\n","                graph.add_relationship(source_node_id=doc_info_id_node,\n","                                       target_node_id=paragraph_id_node,\n","                                       rel_type='СОДЕРЖАНИЕ')\n","\n","            # Удаление их текста номера пункта\n","            text = text[len(prepare_paragraph):]\n","\n","            # Суммаризация текста до заданной длины чанка\n","            if len(text) > chunk_size:\n","                text = summary_text(text, chunk_size, summary_tokenizer, summary_model)\n","\n","            # Принятие мер, если суммаризированный текст слишком велик\n","            if len(text) > chunk_size:\n","                # Разделение текста на предложения\n","                text = sent_tokenize(text)\n","                # Если текст это одно предложение, то оно делится по середине, согласно пунктуации\n","                if len(text) == 1:\n","                    text = split_text_by_punctuation(text[0], chunk_size)\n","\n","            if isinstance(text, str):\n","                if filling_in_graph([Document(page_content=text)], paragraph_id_node, graph, models):\n","                    update_log(log_file, filename, prepare_title, prepare_paragraph, processed_log)\n","            elif isinstance(text, list):\n","                for text_small in text:\n","                    # Последняя попытка суммаризации большого предложения\n","                    if len(text_small) > chunk_size:\n","                        text_small = summary_text(text_small, chunk_size, summary_tokenizer, summary_model)\n","                        \n","                    if filling_in_graph([Document(page_content=text_small)], paragraph_id_node, graph, models):\n","                        update_log(log_file, filename, prepare_title, prepare_paragraph, processed_log)\n","    \n","    break # Обработать один документ\n","\n","# Закрываем соединение с графом\n","graph.close()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":5}
