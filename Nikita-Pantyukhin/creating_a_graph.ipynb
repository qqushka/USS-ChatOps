{"cells":[{"cell_type":"code","execution_count":null,"id":"8bcaa156","metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1721115323021,"user":{"displayName":"qqushka","userId":"02057807309569807044"},"user_tz":-180},"id":"8bcaa156"},"outputs":[],"source":["import os  # Для работы с директориями и переменными в окружении\n","import re  # Для работы с регулярными выражениями"]},{"cell_type":"markdown","id":"c86bd367","metadata":{"id":"c86bd367"},"source":["# 1 - Парсинг текста из PDF"]},{"cell_type":"code","execution_count":null,"id":"7b1c4e6f","metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1721115337175,"user":{"displayName":"qqushka","userId":"02057807309569807044"},"user_tz":-180},"id":"7b1c4e6f"},"outputs":[],"source":["# Путь к PDF документов\n","folder_path_to_pdf = r\"path\\to\\NPA\""]},{"cell_type":"markdown","id":"9aed4c48","metadata":{"id":"9aed4c48"},"source":["## 1.1 - Использование PyMuPDF (fitz) для получения текста"]},{"cell_type":"code","execution_count":null,"id":"01561e36","metadata":{"executionInfo":{"elapsed":13231,"status":"ok","timestamp":1721115337170,"user":{"displayName":"qqushka","userId":"02057807309569807044"},"user_tz":-180},"id":"01561e36"},"outputs":[],"source":["import fitz  # Для чтения текста из PDF"]},{"cell_type":"code","execution_count":null,"id":"008ff2f9","metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1721115363191,"user":{"displayName":"qqushka","userId":"02057807309569807044"},"user_tz":-180},"id":"008ff2f9"},"outputs":[],"source":["def extract_text_from_pdf(pdf_path):\n","    text = \"\"\n","    try:\n","        with fitz.open(pdf_path) as doc:\n","            for page_num in range(len(doc)):\n","                page = doc.load_page(page_num)\n","                text += page.get_text()\n","    except Exception as e:\n","        print(f\"Ошибка извлечения текста из {pdf_path}: {e}\")\n","\n","    return text"]},{"cell_type":"markdown","id":"bab282b9","metadata":{"id":"bab282b9"},"source":["## 1.2 - Определение оглавления и основного текста документа"]},{"cell_type":"code","execution_count":null,"id":"fdb58c3e","metadata":{},"outputs":[],"source":["def separate_text_from_toc(text):\n","    # Паттерн для разделения документа\n","    match = re.compile(r'(.*?)(?:(?:У(?:ТВЕРЖДЕН|твержден)(?:(?:А|а)|(?:Ы|ы)))\\s+приказом\\s+ФСТЭК\\s+России|'\n","                       r'Зарегистрировано\\s+в\\s+Министерстве\\s+юстиции\\s+Российской\\s+Федерации|'\n","                       r'Одобрен\\s+Советом\\s+Федерации|Директор|Правительств(?:а|о))(.*)', re.DOTALL).search(text)\n","    if match:\n","        table_of_contents = match.group(1).strip()\n","        main_text = match.group(2).strip()\n","    else:\n","        table_of_contents = \"\"\n","        main_text = text.strip()\n","\n","    return table_of_contents, main_text"]},{"cell_type":"markdown","id":"2e23cb20","metadata":{},"source":["## 1.3 - Очистка текста от лишней и повторяющейся информации"]},{"cell_type":"code","execution_count":null,"id":"8f67c712","metadata":{},"outputs":[],"source":["# Словарь для преобразования русских месяцев в числовые\n","months = {\n","    'января': '01',\n","    'февраля': '02',\n","    'марта': '03',\n","    'апреля': '04',\n","    'мая': '05',\n","    'июня': '06',\n","    'июля': '07',\n","    'августа': '08',\n","    'сентября': '09',\n","    'октября': '10',\n","    'ноября': '11',\n","    'декабря': '12'\n","}\n","\n","def convert_russian_date(russian_date):\n","    day, month, year, _ = russian_date.split()\n","    return f'{day.zfill(2)}.{months[month]}.{year[:-2]}'\n","\n","def clean_text(text, doc_type, doc_date, doc_number):\n","    # Компиляция регулярного выражения для удаления идентификации документа от КонсультантПлюс\n","    pattern = re.compile(\n","        rf'{doc_type[:1].upper() + doc_type[1:].lower()} .*?{convert_russian_date(doc_date)}.*?N\\s*{doc_number}\\s*\\(ред\\.\\s*от\\s*\\d{{2}}\\.\\d{{2}}\\.\\d{{4}}\\)\\s*?\".*?(?:\\.\\.\\.|\")',\n","        re.DOTALL\n","    )\n","\n","    # Применение регулярного выражения\n","    text = re.sub(pattern, '', text).strip()\n","\n","    # Удаление ненужных фраз и информации от АО \"Кодекс\"\n","    text = re.split(\n","        r'Электронный\\s+текст\\s+документа\\s+подготовлен\\s+АО\\s+\"Кодекс\"\\s+и\\s+сверен\\s+по:',\n","        text,\n","        flags=re.IGNORECASE\n","    )[0]\n","\n","    # Удаление ненужных фраз и информации от КонсультантПлюс\n","    text = re.sub(\n","        r'(Документ предоставлен|www\\.consultant\\.ru|КонсультантПлюс|надежная\\s+правовая\\s+поддержка|Страница\\s*\\d+\\s*из\\s*\\d+\\s|Дата сохранения:\\s+\\d{2}\\.\\d{2}\\.\\d{4})',\n","        '',\n","        text,\n","        flags=re.IGNORECASE\n","    ).strip()\n","\n","    # Разделение текста на строки и удаление пустых строк\n","    lines_text = [line.strip() for line in text.split('\\n') if line.strip()]\n","\n","    # Удаление подчеркиваний и дефисов\n","    lines_text = [re.sub(r'(_+|-+)', '', line) for line in lines_text]\n","\n","    # Объединение очищенных строк в одну строку\n","    cleaned_text = ' '.join(lines_text)\n","    \n","    return cleaned_text"]},{"cell_type":"markdown","id":"15A_tP52wAvJ","metadata":{"id":"15A_tP52wAvJ"},"source":["## 1.4 - Извлечение идентификационных данных документа\n","- Оранжевое - организация;\n","- Фиолетовое - тип документа;\n","- Голубое - дата и номер документа.\n","\n","<img src=\"figures/document_identification.jpg\"/>"]},{"cell_type":"code","execution_count":null,"id":"r_q-1-bzwAAp","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1721130160942,"user":{"displayName":"qqushka","userId":"02057807309569807044"},"user_tz":-180},"id":"r_q-1-bzwAAp","outputId":"3954c1d7-6563-4c85-e9de-3a7d5031192f"},"outputs":[],"source":["def document_identification(text):\n","    # Удаляем строки, содержащие \"Зарегистрировано в Минюсте России\"\n","    text = re.sub(r'Зарегистрировано\\s+в\\s+Минюсте\\s+России\\s+\\d{1,2}\\s+\\w+\\s+\\d{4}\\s+г\\.\\s+N\\s+\\d+', '', text)\n","    \n","    # Паттерны для извлечения информации о документе\n","    document_pattern = re.compile(\n","        r\"(?:(\\d{1,2}\\s+\\w+\\s+\\d{4}\\s+г(?:\\.|ода)?)\\s+N\\s+(\\d+).*(ФЗ)?|)\\s*\"\n","        r\"(ФЕДЕРАЛЬНАЯ\\s+СЛУЖБА\\s+(?:БЕЗОПАСНОСТИ\\s+РОССИЙСКОЙ\\s+ФЕДЕРАЦИИ|ПО\\s+ТЕХНИЧЕСКОМУ\\s+И\\s+ЭКСПОРТНОМУ\\s+КОНТРОЛЮ)|\"\n","        r\"ПРАВИТЕЛЬСТВО\\s+РОССИЙСКОЙ\\s+ФЕДЕРАЦИИ|РОССИЙСКАЯ\\s+ФЕДЕРАЦИЯ|ПРЕЗИДЕНТ\\s+РОССИЙСКОЙ\\s+ФЕДЕРАЦИИ)\\s*\"\n","        r\"((?:ПРИКАЗ|ПОСТАНОВЛЕНИЕ|УКАЗ|ФЕДЕРАЛЬНЫЙ\\s+ЗАКОН))(?:\\s+от\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4}\\s+г(?:\\.|ода)?)\\s+(?:N|№)\\s+(\\d+)|)\"\n","    )\n","\n","    # Примеры:\n","    #   ФЕДЕРАЛЬНАЯ СЛУЖБА БЕЗОПАСНОСТИ РОССИЙСКОЙ ФЕДЕРАЦИИ ПРИКАЗ от 6 мая 2019 года N 196 Об утверждении Требований к средс...\n","    #   ПРАВИТЕЛЬСТВО РОССИЙСКОЙ ФЕДЕРАЦИИ ПОСТАНОВЛЕНИЕ от 8 февраля 2018 г. № 127 МОСКВА Об утверждении Правил категорирован...\n","    #   26 июля 2017 года N 187ФЗ РОССИЙСКАЯ ФЕДЕРАЦИЯ ФЕДЕРАЛЬНЫЙ ЗАКОН О БЕЗОПАСНОСТИ КРИТИЧЕСКОЙ ИНФОРМАЦИОННОЙ ИНФРАСТРУКТ...\n","    #   ПРЕЗИДЕНТ РОССИЙСКОЙ ФЕДЕРАЦИИ УКАЗ от 30 марта 2022 г. N 166 О МЕРАХ ПО ОБЕСПЕЧЕНИЮ ТЕХНОЛОГИЧЕСКОЙ НЕЗАВИСИМОСТИ И Б...\n","\n","    # Ищем соответствия в тексте\n","    document_match = document_pattern.search(text)\n","\n","    if document_match:\n","        if document_match.group(1) is None:\n","            organization = document_match.group(4)\n","            document_type = document_match.group(5)\n","            date = document_match.group(6)\n","            document_number = document_match.group(7)\n","        else:\n","            organization = document_match.group(4)\n","            document_type = document_match.group(5)\n","            date = document_match.group(1)\n","            document_number = document_match.group(2)\n","\n","        return {\n","            'organization': organization,\n","            'document_type': document_type,\n","            'date': date,\n","            'document_number': document_number\n","        }\n","    else:\n","        print(\"Не удалось извлечь информацию из текста\")\n","        return None"]},{"cell_type":"markdown","id":"4dz1wxsR6uQn","metadata":{"id":"4dz1wxsR6uQn"},"source":["## 1.5 - Извлечение заголовков и (под)пунктов\n","Используется счетчик для корректировки записи распознанного массива (под)пунктов."]},{"cell_type":"code","execution_count":null,"id":"e_f1YSS06tRE","metadata":{"executionInfo":{"elapsed":375,"status":"ok","timestamp":1721128875634,"user":{"displayName":"qqushka","userId":"02057807309569807044"},"user_tz":-180},"id":"e_f1YSS06tRE"},"outputs":[],"source":["def extract_titles_and_texts(text):\n","    \"\"\"\n","    Извлекает заголовки разделов с римскими цифрами, \"Статья X\" и заголовки по шаблону \"Приложение N X\".\n","    \"\"\"\n","    roman_pattern = re.compile(r\"(?<=\\b)([IVXLCDM]+\\.\\s+.+?)(?=\\s+[IVXLCDM]+\\.\\s|\\s*\\d+\\.\\s|$)\", re.MULTILINE | re.DOTALL)\n","    article_pattern = re.compile(r\"(?<=\\b)(Статья\\s+\\d+\\.\\s+.+?)(?=\\s+Статья\\s+\\d+\\.\\s|\\s*\\d+(?:\\.|\\))\\s|$)\", re.MULTILINE | re.DOTALL)\n","    appendix_order_pattern = re.compile(r\"(?<=\\b)(Приложение\\s+N\\s+\\d+\\s+к\\s+приказу\\s+ФСБ\\s+России\\s+от\\s+\\d+\\s+\\w+\\s+\\d{4}\\s+г(?:\\.|ода)\\s+N\\s+\\d+)(?=\\s+)\", re.MULTILINE | re.DOTALL)\n","\n","    matches = list(re.finditer(roman_pattern, text)) + list(re.finditer(article_pattern, text)) + list(re.finditer(appendix_order_pattern, text))\n","    matches.sort(key=lambda x: x.start())\n","\n","    results = []\n","    for i, match in enumerate(matches):\n","        title = match.group(1).strip()\n","        start_pos = match.end()\n","\n","        end_pos = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n","        results.append((title, text[start_pos:end_pos].strip()))\n","\n","    return results\n","\n","def get_item_numbers(text):\n","    \"\"\"\n","    Извлекает номера пунктов из текста.\n","    \"\"\"\n","    pattern = re.compile(r\"((?:\\d*\\.)*)\\s*\")\n","    match = pattern.match(text)\n","    if match:\n","        return match.group(1).strip().rstrip('.').split('.')\n","    \n","    return []\n","\n","def extract_subpoints(text, counter_status=['0'], register_counter_status=0):\n","    \"\"\"\n","    Извлекает подпункты из текста и проверяет их счетчиком.\n","    \"\"\"\n","    \n","    def update_counters(subpoint_number, counter_status, register_counter_status):\n","        \"\"\"\n","        Обновляет счетчики для подуровней разделов.\n","        \"\"\"\n","        if len(subpoint_number) > len(counter_status):\n","            register_counter_status += 1\n","            counter_status.append('0')\n","        elif len(subpoint_number) < len(counter_status):\n","            register_counter_status -= 1\n","            counter_status.pop()\n","\n","        counter_status[register_counter_status] = str(int(counter_status[register_counter_status]) + 1)\n","\n","        return counter_status, register_counter_status\n","\n","    subpoint_pattern = re.compile(r\"((?:\\d+\\.)+\\s*.+?)(?=(?:\\d+\\.)+|$)\", re.DOTALL)\n","    matches = re.findall(subpoint_pattern, text)\n","    subpoints = []\n","\n","    first_register_occupied = False\n","    \n","    for match in matches:\n","        subpoint = match.strip()\n","        subpoint_number = get_item_numbers(subpoint)\n","        if subpoint_number:\n","            counter_status, register_counter_status = update_counters(subpoint_number, counter_status, register_counter_status)\n","\n","            # Логирование\n","            # print(f\"Счетчик: {counter_status}, Распознано: {subpoint_number}, Уровень подпункта: {register_counter_status}\")\n","\n","            if not first_register_occupied:\n","                if subpoint_number == counter_status:\n","                    first_register_occupied = True\n","                else:\n","                    counter_status[register_counter_status] = str(int(counter_status[register_counter_status]) - 1)\n","            if first_register_occupied:\n","                if subpoint_number != counter_status:\n","                    if subpoints:\n","                        subpoints[-1] += ' ' + subpoint\n","                        counter_status[register_counter_status] = str(int(counter_status[register_counter_status]) - 1)\n","                else:\n","                    subpoints.append(subpoint)\n","        \n","    return subpoints, counter_status, register_counter_status"]},{"cell_type":"markdown","id":"faf21fca","metadata":{"id":"faf21fca"},"source":["## 1.6 - Получение информации из PDF"]},{"cell_type":"code","execution_count":null,"id":"6b7ce7b9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":420,"status":"ok","timestamp":1721130181365,"user":{"displayName":"qqushka","userId":"02057807309569807044"},"user_tz":-180},"id":"6b7ce7b9","outputId":"cf1b98c8-90bd-4e21-8f5a-ee1cdfa12ec5"},"outputs":[],"source":["def process_pdf_file(text, doc_info):\n","    \"\"\"\n","    Обрабатывает PDF файл: извлекает текст, разделяет его на оглавление и основной текст,\n","    идентифицирует документ, извлекает заголовки и их подпункты.\n","    \"\"\"\n","    titles_and_subpoints = {}\n","\n","    # Поиск заголовков\n","    titles = extract_titles_and_texts(text)\n","\n","    # Инициализация счетчиков подпунктов\n","    counter_subpoints = ['0']\n","    register_counter_status = 0\n","\n","    len_text_render = 50\n","    if titles:\n","        for title, content in titles:\n","            if re.match(r\"Статья\\s+\\d+\\.\", title) or re.match(r\"Приложение\\s+N\\s+\\d+\", title):\n","                # Обнуляем счетчики\n","                counter_subpoints = ['0']\n","                register_counter_status = 0\n","\n","                # Извлекаем нужный текст заголовка\n","                if re.match(r\"Приложение\\s+N\\s+\\d+\", title):\n","                    title = re.match(r\"Приложение\\s+N\\s+\\d+\", title).group()\n","            \n","            print(f\"\\t({len(title)}) {title}\")\n","            subpoints, counter_subpoints, register_counter_status = extract_subpoints(content, counter_subpoints, register_counter_status)\n","            titles_and_subpoints[title] = subpoints\n","            for subpoint in subpoints:\n","                print(f\"\\t\\t({len(subpoint)}) {subpoint[:len_text_render]} [..]\")\n","    else:\n","        subpoints, counter_subpoints, register_counter_status = extract_subpoints(text, counter_subpoints, register_counter_status)\n","        for subpoint in subpoints:\n","            print(f\"\\t({len(subpoint)}) {subpoint[:len_text_render]} [..]\")\n","        titles_and_subpoints[\"Без заголовка\"] = subpoints\n","\n","    return {\n","        'doc_info': doc_info,\n","        'titles_and_subpoints': titles_and_subpoints,\n","        'text': text\n","    }\n","\n","def extract_info_from_pdfs(folder_path):\n","    \"\"\"\n","    Извлекает информацию из всех PDF файлов в указанной папке.\n","    \"\"\"\n","    pdf_texts = {}\n","    for filename in os.listdir(folder_path):\n","        # Фильтр для вывода хорошо предобработанных докуменатов:\n","        # if filename.endswith(\".pdf\") and ('FSB' in filename or 'FSTEK' in filename) and 'FSTEK239' not in filename and 'FSTEK75' not in filename\n","        if filename.endswith(\".pdf\") and ('FSB' in filename or 'FSTEK' in filename) and 'FSTEK239' not in filename and 'FSTEK75' not in filename:\n","            pdf_file = os.path.join(folder_path, filename)\n","            print(filename)\n","            text = extract_text_from_pdf(pdf_file)\n","            if text:\n","                doc_info, text = separate_text_from_toc(text)\n","                doc_info = ' '.join([line.strip() for line in doc_info.split('\\n') if line.strip()])\n","                doc_info = document_identification(doc_info)\n","                print(f\"Орг: {doc_info['organization']}\\nТип: {doc_info['document_type']}\\nДата: {doc_info['date']}\\nНомер: {doc_info['document_number']}\\n\")\n","\n","                text = clean_text(text, doc_info['document_type'], doc_info['date'], doc_info['document_number'])\n","                if text:\n","                    pdf_data = process_pdf_file(text, doc_info)\n","                    pdf_texts[filename] = {\n","                        'filename': filename,\n","                        **pdf_data\n","                    }\n","                    print('-' * 100)\n","            \n","    return pdf_texts\n","\n","pdf_texts = extract_info_from_pdfs(folder_path_to_pdf)"]},{"cell_type":"markdown","id":"5fe34bae","metadata":{"id":"5fe34bae"},"source":["# 2 - Подготовка, создание графа"]},{"cell_type":"markdown","id":"4446fc3a","metadata":{},"source":["## 2.1 - Подготовка"]},{"cell_type":"markdown","id":"e15645de","metadata":{},"source":["### 2.1.1 - Разделение текста на предложения\n","Для разделения большого текста на предложения используется функция `sent_tokenize` и набор пунктуации из библиотеки `nltk`.\n","\n","Если весь текст это предложение, то он будет разбит на чанки по знакам пунктуации."]},{"cell_type":"code","execution_count":null,"id":"604e54b0","metadata":{},"outputs":[],"source":["import nltk\n","from nltk.tokenize import sent_tokenize"]},{"cell_type":"code","execution_count":null,"id":"6f159631","metadata":{},"outputs":[],"source":["# Набор пунктуации nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"id":"5fe6e52a","metadata":{},"outputs":[],"source":["def split_text_by_punctuation(text, max_chunk_size):\n","    \"\"\"\n","    Разделяет текст примерно в середине, с учетом размера чанка и пунктуации.\n","    \"\"\"\n","    def find_split_index(text, max_chunk_size):\n","        # Найти индекс для разрыва текста около середины, но не превышая max_chunk_size\n","        mid = len(text) // 2\n","        # Найти ближайшую пунктуацию к середине текста\n","        split_points = re.finditer(r'[.,:;!?]\\s*', text)\n","        best_split = None\n","        for point in split_points:\n","            if point.start() <= mid <= point.end():\n","                best_split = point.end()\n","                break\n","            if point.start() < mid:\n","                best_split = point.end()\n","            else:\n","                break\n","            \n","        return best_split if best_split and best_split <= max_chunk_size else mid\n","\n","    chunks = []\n","    while len(text) > max_chunk_size:\n","        split_index = find_split_index(text, max_chunk_size)\n","        chunks.append(text[:split_index].strip())\n","        text = text[split_index:].strip()\n","    \n","    chunks.append(text)\n","\n","    return chunks"]},{"cell_type":"markdown","id":"e24c7c8b","metadata":{},"source":["### 2.1.2 - Суммаризация (Модель IlyaGusev/mbart_ru_sum_gazeta)\n","Если длина предложения превысит размер чанка, то из текста предложения достаётся смысловая выжимка."]},{"cell_type":"markdown","id":"29aae50b","metadata":{},"source":["Пример использования:\n"," - Пусть `article_text` это текст для обработки.\n","\n","`input_ids = tokenizer([article_text], max_length=600, truncation=True, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")`\n","\n","`output_ids = model.generate(input_ids=input_ids, no_repeat_ngram_size=4)[0]`\n","\n","`summary = tokenizer.decode(output_ids, skip_special_tokens=True)`"]},{"cell_type":"code","execution_count":null,"id":"e8f20fd5","metadata":{},"outputs":[],"source":["# Для инициализации модели для суммаризации текста\n","from transformers import MBartTokenizer, MBartForConditionalGeneration"]},{"cell_type":"code","execution_count":null,"id":"f9bf5c53","metadata":{},"outputs":[],"source":["summary_model_name = \"IlyaGusev/mbart_ru_sum_gazeta\"\n","\n","summary_tokenizer = MBartTokenizer.from_pretrained(summary_model_name)  # Скачивание модели\n","summary_model = MBartForConditionalGeneration.from_pretrained(summary_model_name)  # Загрузка модели\n","summary_model.to(\"cuda\")"]},{"cell_type":"code","execution_count":null,"id":"b9e4cbab","metadata":{},"outputs":[],"source":["def summary_text(text, chunk_size, summary_tokenizer, summary_model):\n","    \"\"\"\n","    Сжимает текст до размера чанка, используя модель суммаризации.\n","    \"\"\"\n","    len_summary = chunk_size + 1\n","    previous_len = len(text)\n","    no_change_count = 0\n","\n","    while len_summary > chunk_size and no_change_count < 3:\n","        input_ids = summary_tokenizer(\n","            [text],\n","            max_length=600,\n","            truncation=True,\n","            return_tensors=\"pt\",\n","        )[\"input_ids\"].to(\"cuda\")\n","        output_ids = summary_model.generate(\n","            input_ids=input_ids,\n","            no_repeat_ngram_size=4\n","        )[0]\n","        new_text = summary_tokenizer.decode(output_ids, skip_special_tokens=True)\n","        \n","        if len(new_text) >= previous_len:\n","            no_change_count += 1\n","        else:\n","            no_change_count = 0\n","        \n","        text = new_text\n","        len_summary = len(text)\n","        previous_len = len(text)\n","    \n","    return text"]},{"cell_type":"markdown","id":"86dc3702","metadata":{},"source":["### 2.1.3 - Доступ к LLM через API от HuggingFace (mistralai/Mixtral-8x7B-Instruct-v0.1)\n","Инициализация LLMs для дальнейшего использования этого в функции, которая конвертирует текст в узлы и отношения между ними.\n","\n","Нужен [API ключ](https://huggingface.co/settings/tokens) в режиме `WRITE`.\n","\n","Выбранные модели:\n","1) mistralai/Mistral-7B-Instruct-v0.1\n","2) mistralai/Mistral-7B-Instruct-v0.2\n","3) mistralai/Mistral-7B-Instruct-v0.3\n","4) mistralai/Mixtral-8x7B-Instruct-v0.1\n","5) meta-llama/Meta-Llama-3-8B-Instruct"]},{"cell_type":"code","execution_count":null,"id":"6393053f","metadata":{},"outputs":[],"source":["from getpass import getpass  # Для хранения секретных ключей\n","\n","# Для инициализации и работы с LLM\n","from langchain_community.llms import HuggingFaceEndpoint\n","from langchain_experimental.graph_transformers import LLMGraphTransformer"]},{"cell_type":"code","execution_count":null,"id":"d5f26e32","metadata":{},"outputs":[],"source":["os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass(prompt=\"Введите API ключ от HuggingFaceHub\")"]},{"cell_type":"code","execution_count":null,"id":"84f36199","metadata":{},"outputs":[],"source":["models = [\n","    \"mistralai/Mistral-7B-Instruct-v0.1\",\n","    \"mistralai/Mistral-7B-Instruct-v0.2\",\n","    \"mistralai/Mistral-7B-Instruct-v0.3\",\n","    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n","    \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","]\n","\n","def initialize_endpoint(model_id):\n","    return HuggingFaceEndpoint(repo_id=model_id)\n","\n","def initialize_transformer(model_id):\n","    hg_llm = HuggingFaceEndpoint(repo_id=model_id)\n","    return LLMGraphTransformer(llm=hg_llm)"]},{"cell_type":"markdown","id":"d2b7a578","metadata":{},"source":["### 2.1.4 - Работа с Neo4j в AuraDB\n","Граф собирается при использовании Neo4j через облачную версию [AuraDB](https://neo4j.com/cloud/platform/aura-graph-database/)."]},{"cell_type":"code","execution_count":null,"id":"c3ac0ce5","metadata":{},"outputs":[],"source":["from neo4j import GraphDatabase  # Для подключения к среде Neo4j"]},{"cell_type":"code","execution_count":null,"id":"63b0f1e4","metadata":{},"outputs":[],"source":["# Данные для входа в среду Neo4j\n","os.environ[\"NEO4J_URI\"] = \"neo4j+s://...\"\n","os.environ[\"NEO4J_USERNAME\"] = \"...\"\n","os.environ[\"NEO4J_PASSWORD\"] = \"...\""]},{"cell_type":"code","execution_count":null,"id":"c815c012","metadata":{},"outputs":[],"source":["class InteractionNeo4jGraph:\n","    def __init__(self):\n","        self.driver = GraphDatabase.driver(\n","            os.getenv(\"NEO4J_URI\"),\n","            auth=(os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n","        )\n","\n","    def close(self):\n","        \"\"\"\n","        Завершает работу с Neo4j.\n","        \"\"\"\n","        self.driver.close()\n","\n","    def create_node(self, node_text, node_type, properties={}):\n","        \"\"\"\n","        Добавляет узел, независимо от существующих в среде.\n","        Если узел с похожими параметрами уже есть, то создается клон с другим <id>.\n","        \"\"\"\n","        with self.driver.session() as session:\n","            result = session.run(f\"\"\"\n","                CREATE (n:{node_type} {{text: $node_text}})\n","                SET n += $properties\n","                RETURN elementId(n) AS node_id\n","            \"\"\", node_text=node_text, properties=properties)\n","            return result.single()[\"node_id\"]\n","\n","    def addition_node(self, node_text, node_type, properties={}):\n","        \"\"\"\n","        Добавляет узел, в зависимости от существующих в среде.\n","        Если узел с похожими параметрами уже есть, то функция обновит их на новые.\n","        \"\"\"\n","        with self.driver.session() as session:\n","            result = session.run(f\"\"\"\n","                MERGE (n:{node_type} {{text: $node_text}})\n","                SET n += $properties\n","                RETURN elementId(n) AS node_id\n","            \"\"\", node_text=node_text, properties=properties)\n","            return result.single()[\"node_id\"]\n","\n","    def add_relationship(self, source_node_id, target_node_id, rel_type):\n","        \"\"\"\n","        Добавляет отношения между узлами.\n","        \"\"\"\n","        with self.driver.session() as session:\n","            session.run(f\"\"\"\n","                MATCH (a) WHERE elementId(a) = $source_node_id\n","                MATCH (b) WHERE elementId(b) = $target_node_id\n","                MERGE (a)-[r:{rel_type}]->(b)\n","            \"\"\", source_node_id=source_node_id, target_node_id=target_node_id)\n","\n","    def find_nodes_by_text(self, node_text):\n","        \"\"\"\n","        Поиск узлов по полностью указанному тексту в них.\n","        \"\"\"\n","        with self.driver.session() as session:\n","            result = session.run(f\"\"\"\n","                MATCH (n {{text: $node_text}})\n","                RETURN n\n","            \"\"\", node_text=node_text)\n","            return [record[\"n\"] for record in result]\n","\n","    def find_similar_nodes_by_text(self, node_text):\n","        \"\"\"\n","        Поиск узлов по частично указанному тексту в них.\n","        \"\"\"\n","        def split_and_trim(text):\n","            \"\"\"\n","            Функция для разбивки текста на части и удаления половины символов у каждой части.\n","            \"\"\"\n","            parts = text.split()\n","            trimmed_parts = [part[:len(part)//2] for part in parts]\n","            return trimmed_parts\n","\n","        # Разбиваем текст и удаляем половину символов у каждой части\n","        trimmed_texts = split_and_trim(node_text)\n","\n","        # Формируем запрос\n","        query = \"MATCH (n) WHERE \"\n","        params = {}\n","        # Создаем условия для поиска узлов по каждому обрезанному тексту\n","        for i, trimmed_text in enumerate(trimmed_texts):\n","            query += f\"n.text CONTAINS $node_text{i}\"\n","            if i < len(trimmed_texts) - 1:\n","                query += \" AND \"\n","            params[f\"node_text{i}\"] = trimmed_text\n","        query += \" RETURN elementId(n) AS node_id\"\n","\n","        # Отправляем запрос\n","        with self.driver.session() as session:\n","            result = session.run(query, **params)\n","            return [record[\"node_id\"] for record in result]\n","\n","    def find_node_id_by_text(self, node_text):\n","        \"\"\"\n","        Поиск ID узла по тексту.\n","        \"\"\"\n","        with self.driver.session() as session:\n","            result = session.run(f\"\"\"\n","                MATCH (n {{text: $node_text}})\n","                RETURN elementId(n) AS node_id\n","            \"\"\", node_text=node_text)\n","            node = result.single()\n","            return node[\"node_id\"] if node else None\n","    \n","    def find_types_node_by_id(self, node_id):\n","        \"\"\"\n","        Получение состояния поля \"типы\" узла по ID. \n","        \"\"\"\n","        with self.driver.session() as session:\n","            result = session.run(f\"\"\"\n","                MATCH (n) WHERE elementId(n) = $node_id\n","                RETURN n.типы AS node_types\n","            \"\"\", node_id=node_id)\n","            record = result.single()\n","            return record[\"node_types\"] if record else None\n","    \n","    def updating_types_node_by_id(self, node_id, properties_types):\n","        \"\"\"\n","        Обновление состояния поля \"типы\" узла по ID.\n","        \"\"\"\n","        with self.driver.session() as session:\n","            session.run(f\"\"\"\n","                MATCH (n) WHERE elementId(n) = $node_id\n","                SET n.типы = $properties_types\n","            \"\"\", node_id=node_id, properties_types=properties_types)\n","    \n","    def find_text_node_by_id(self, node_id):\n","        \"\"\"\n","        Получение состояния поля \"text\" узла по ID. \n","        \"\"\"\n","        with self.driver.session() as session:\n","            result = session.run(f\"\"\"\n","                MATCH (n) WHERE elementId(n) = $node_id\n","                RETURN n.text AS node_text\n","            \"\"\", node_id=node_id)\n","            record = result.single()\n","            return record[\"node_text\"] if record else None\n","    \n","    def check_current_node_type_by_id(self, node_id):\n","        \"\"\"\n","        Получение типа узла по ID. \n","        \"\"\"\n","        with self.driver.session() as session:\n","            result = session.run(f\"\"\"\n","                MATCH (n) WHERE elementId(n) = $node_id\n","                RETURN labels(n) AS node_type\n","            \"\"\", node_id=node_id)\n","            record = result.single()\n","            return record[\"node_type\"] if record else None"]},{"cell_type":"markdown","id":"af9776b6","metadata":{},"source":["### 2.1.5 - Перевод текста на русский"]},{"cell_type":"code","execution_count":null,"id":"bacedeca","metadata":{},"outputs":[],"source":["from deep_translator import GoogleTranslator  # Для перевода текста на русский"]},{"cell_type":"code","execution_count":null,"id":"402116e5","metadata":{},"outputs":[],"source":["def translate_to_rus(text):\n","    # Проверяем, содержит ли текст английские буквы\n","    if re.search(r'[a-zA-Z]', text):\n","        return GoogleTranslator(source='en', target='ru').translate(text)\n","    else:\n","        return text\n","def translate_to_en(text):\n","    # Проверяем, содержит ли текст английские буквы\n","    if re.search(r'[а-яёА-ЯЁ]', text):\n","        return GoogleTranslator(source='ru', target='en').translate(text)\n","    else:\n","        return text"]},{"cell_type":"markdown","id":"025f4205","metadata":{},"source":["### 2.1.6 - Подготовка ID и типа узла перед добавлением "]},{"cell_type":"code","execution_count":null,"id":"bb61db14","metadata":{},"outputs":[],"source":["def prepare_text_node(text):\n","    \"\"\"\n","    Подготовка текста для вставки в узел.\n","\n","    Пример return: \"Тест текст\"\n","    \"\"\"\n","    \n","    # Проверка, является ли текст заголовком\n","    if re.match(r'^[IVXLCDM]+\\.\\s+', text, re.IGNORECASE) or \\\n","       re.match(r\"Статья\\s+\\d+\\.\", text, re.IGNORECASE) or \\\n","       re.match(r\"Приложение\\s+N\\s+\\d+\", text, re.IGNORECASE):\n","        return text\n","\n","    # Превращение возможных разделяющих знаков в пробелы\n","    text = re.sub(r'[-_]', ' ', text)\n","    # Удаление лишних пробелов и проверка наличия текста\n","    text = text.strip()\n","    if not text:\n","        return None\n","    # Вставка пробелов между маленькими и большими буквами\n","    text = re.sub(r'([a-zа-яё])([A-ZА-ЯЁ])', r'\\1 \\2', text).strip()\n","    # Попытка перевести текст\n","    text = translate_to_rus(text)\n","    # Форматирование аббревиатуры\n","    text = re.sub(r'(?:Gos SOPKA|Гос СОПКА|Госсопка|Гос сопка)', 'ГосСОПКА', text)\n","\n","    # Разделение текста на слова\n","    words = text.split(' ')\n","    formatted_words = []\n","    \n","    # Обработка первого слова\n","    if words:\n","        if re.search(r'[А-ЯЁA-Z]{2,}', words[0]):\n","            first_word = words[0]\n","        else:\n","            first_word = words[0].capitalize()\n","        formatted_words.append(first_word)\n","    \n","    # Обработка оставшихся слов\n","    for word in words[1:]:\n","        if re.search(r'[А-ЯЁA-Z]{2,}', word):\n","            formatted_words.append(word)\n","        else:\n","            formatted_words.append(word.lower())\n","\n","    return ' '.join(formatted_words)\n","\n","def prepare_type_node(text):\n","    \"\"\"\n","    Подготовка названия типа узла.\n","\n","    Пример return: \"ТестТекст\"\n","    \"\"\"\n","\n","    # Проверка на принадлежность к собственному набору\n","    if re.match(r'(?:Организация|НомерДокумента|Заголовок|Пункт)', text):\n","        return text\n","\n","    # Превращение возможных разделяющих знаков в пробелы\n","    text = re.sub(r'[-_]', ' ', text)\n","    # Удаление лишних пробелов и проверка наличия текста\n","    text = text.strip()\n","    if not text:\n","        return None\n","    # Вставка пробелов между маленькими и большими буквами\n","    text = re.sub(r'([a-zа-яё])([A-ZА-ЯЁ])', r'\\1 \\2', text).strip()\n","    # Попытка перевести текст\n","    text = translate_to_rus(text).replace('-', ' ')\n","    # Приведение к общему виду\n","    text = text.capitalize()\n","\n","    # Разделение текста на слова, приведение каждого слова к нижнему регистру, кроме первого символа\n","    text = ''.join([word.capitalize() for word in text.split()])\n","\n","    return text"]},{"cell_type":"markdown","id":"afbe3bbb","metadata":{},"source":["### 2.1.7 - Лемматизация слов\n","Для поиска похожих узлов и исключения случаев добавление по сути одинаковых узлов, но отличающихся только их окончанием."]},{"cell_type":"code","execution_count":null,"id":"81ca89ad","metadata":{},"outputs":[],"source":["from pymorphy2 import MorphAnalyzer  # Для морфологического анализа слов"]},{"cell_type":"code","execution_count":null,"id":"3e8ac961","metadata":{},"outputs":[],"source":["# Инициализация морфологического анализатора\n","morph = MorphAnalyzer()\n","\n","def lemmatize_word(word):\n","    # Приведение слова к начальной форме\n","    p = morph.parse(word)[0]\n","    return p.normal_form\n","\n","def lemmatize_text(text):\n","    # Разделение текста на слова и лемматизация каждого слова\n","    words = text.split()\n","    lemmatized_words = [lemmatize_word(word) for word in words]\n","    lemmatized_words = ' '.join(lemmatized_words)\n","    return lemmatized_words[:1].upper() + lemmatized_words[1:]"]},{"cell_type":"markdown","metadata":{},"source":["## 2.2 - Создание RAG графа"]},{"cell_type":"code","execution_count":null,"id":"6b4d8f16","metadata":{},"outputs":[],"source":["from langchain_core.documents import Document  # Для конвертации текста в допустимый формат функции \"convert_to_graph_documents\"\n","import json_repair  # Для работы функции \"convert_to_graph_documents\"\n","\n","import json  # Для логирования успешно завершенных этапов"]},{"cell_type":"code","execution_count":null,"id":"WxXy5GVrcAg6","metadata":{"id":"WxXy5GVrcAg6"},"outputs":[],"source":["# Длинна чанка текста поступающего в функцию \"convert_to_graph_documents\"\n","chunk_size = 500"]},{"cell_type":"markdown","id":"fe8a3d0a","metadata":{},"source":["### 2.2.1 - Логирование\n","Это поможет избежать дублирования узлов при перезапуске ноутбука."]},{"cell_type":"code","execution_count":null,"id":"8397303a","metadata":{},"outputs":[],"source":["# Загружаем существующий лог или создаём новый\n","log_file = 'processed_files_log.json'\n","if os.path.exists(log_file):\n","    with open(log_file, 'r', encoding='utf-8') as file:\n","        processed_log = json.load(file)\n","else:\n","    processed_log = {}\n","\n","# Функция для обновления лога\n","def update_log(log_file, filename, title, title_id_node, paragraph, paragraph_id_node, processed_log, status):\n","    if filename not in processed_log:\n","        processed_log[filename] = {}\n","        \n","    if title not in processed_log[filename]:\n","        processed_log[filename][title] = []\n","\n","    # Найти запись\n","    entry_title_id_node = next((item for item in processed_log[filename][title] if item['title_id_node'] == title_id_node), None)\n","    if entry_title_id_node:\n","        if paragraph not in entry_title_id_node:\n","            entry_title_id_node[paragraph] = {}\n","        entry_title_id_node[paragraph][\"paragraph_id_node\"] = paragraph_id_node\n","        entry_title_id_node[paragraph][\"status\"] = status\n","    else:\n","        processed_log[filename][title].append({\n","            \"title_id_node\": title_id_node,\n","            paragraph: {\"paragraph_id_node\": paragraph_id_node,\n","                        \"status\": status}\n","        })\n","\n","    with open(log_file, 'w', encoding='utf-8') as file:\n","        json.dump(processed_log, file, ensure_ascii=False, indent=4)"]},{"cell_type":"markdown","id":"1db4fb75","metadata":{},"source":["### 2.2.2 - Получение графа от LLM и заполнение среды Neo4j"]},{"cell_type":"code","execution_count":null,"id":"2eb38b63","metadata":{},"outputs":[],"source":["# Функция для заполнения графа\n","def filling_in_graph(document, paragraph_id_node, graph, models, filter=True):\n","    global hg_llm_transformer\n","    all_models_failed = True\n","    stop = False\n","    \n","    for model_id in models:\n","        try:\n","\n","            # Использование LLM для построения графа:\n","            \n","            if filter:\n","                print(\"Использование фильтра..\")\n","                #   Фильрация:\n","                # Инициализация LLM\n","                hg_llm_endpoint = initialize_endpoint(model_id)\n","                # Фильтрация параметров\n","                hg_llm_transformer = LLMGraphTransformer(\n","                    llm=hg_llm_endpoint,\n","                    allowed_nodes=[\n","                        \"Document\", \"Organization\", \"Person\", \"Group\", \"Information\", \n","                        \"Legislation\", \"System\", \"Security\", \"Process\", \n","                        \"Location\", \"Event\", \"Resources\", \"PoliticalParty\", \"Incident\", \"Decision\"\n","                    ],\n","                    allowed_relationships=[\n","                        \"IN_ACCORDANCE_WITH\", \"INTERACTS_WITH\", \"OWNS\", \"USES\",\n","                        \"LOCATED_IN\", \"RELATED_TO\", \"ENSURES\", \"CONNECTED_TO\", \"HAS_TYPE\",\n","                        \"RESPONSIBLE_FOR\", \"CONTAINS\", \"REGULATES\", \"BELONGS_TO\", \"IS\",\n","                        \"PROVIDES\"\n","                    ]\n","                )\n","                graph_documents = hg_llm_transformer.convert_to_graph_documents(document)\n","            else:\n","                print(\"Обход фильтра..\")\n","                #   Без фильтрации:\n","                # Инициализация LLM\n","                hg_llm_transformer = initialize_transformer(model_id)\n","                graph_documents = hg_llm_transformer.convert_to_graph_documents(document)\n","                stop = True\n","\n","            # Вывод результатов\n","            print(f\"\\n({len(graph_documents[0].nodes)}) Nodes: {graph_documents[0].nodes}\")\n","            print(f\"({len(graph_documents[0].relationships)}) Relationships: {graph_documents[0].relationships}\")\n","            \n","            # Событие когда LLM выводит пустой граф\n","            if len(graph_documents[0].nodes) == 0:\n","                print(f\"\\n{model_id} выдала пустой граф.\\n\")\n","                continue\n","            all_models_failed = False\n","            \n","            # Создание словаря для сохранения ID по тексту в нём\n","            individual_dictionary_nodes_ids = {}\n","            \n","            # Заполнение графа в Neo4j\n","            for graph_document in graph_documents:\n","                # Заполнение в Neo4j распознанных сущностей\n","                for node in graph_document.nodes:\n","                    # Подготовка данных\n","                    prepare_text = prepare_text_node(node.id)\n","                    prepare_type = prepare_type_node(node.type)\n","                    \n","                    # Исключение добавления None сущностей\n","                    if prepare_text is None or prepare_type is None:\n","                        continue\n","                    \n","                    # Проверка на существование такого узла\n","                    find_node_id = graph.find_node_id_by_text(prepare_text)\n","                    individual_dictionary_nodes_ids[prepare_text] = None\n","                    if find_node_id == None:\n","                        # Попытка найти узел с похожим текстом\n","                        similar_nodes_ids = graph.find_similar_nodes_by_text(prepare_text)\n","                        if similar_nodes_ids != []:\n","                            print(f\"Обнаружено возможное совпадение в узлах: `{similar_nodes_ids}`\")\n","                            for similar_node_id in similar_nodes_ids:\n","                                # Проверка на одинаковое содержание\n","                                print(f\"Лемма текущего текста: `{lemmatize_text(prepare_text)}`; Лемма найденного текста: `{lemmatize_text(graph.find_text_node_by_id(similar_node_id))}`\")\n","                                if lemmatize_text(prepare_text) == lemmatize_text(graph.find_text_node_by_id(similar_node_id)):\n","                                    # Если найден узел с очень похожим содержанием, то\n","                                    # создаём отношение: \"Пункт\" -> \"X-старая-сущность\"\n","                                    print(f\"Обнаружен похожий текст: `{prepare_text}`; В узле c ID: `{similar_node_id}`\")\n","                                    graph.add_relationship(source_node_id=paragraph_id_node,\n","                                                   target_node_id=similar_node_id,\n","                                                   rel_type='СОДЕРЖАНИЕ')\n","                                    # Сохранение ID узла\n","                                    individual_dictionary_nodes_ids[prepare_text] = similar_node_id\n","                                    break # Остановка поиска\n","\n","                    # Если такой узел существует то обновляем в уже существующем узле поле \"типы\"    \n","                    else:\n","                        # Поиск уже существующих типов узла\n","                        find_types = graph.find_types_node_by_id(find_node_id)\n","                        if find_types:\n","                            find_types = find_types.split('; ')\n","                        else:\n","                            find_types = []\n","                        # Добавление нового типа\n","                        # Исключение добавления присвоенного типа\n","                        current_node_type = graph.check_current_node_type_by_id(find_node_id)[0]\n","                        if prepare_type not in find_types and current_node_type != prepare_type:\n","                            find_types.append(prepare_type)\n","                            find_types = '; '.join(find_types)\n","                            graph.updating_types_node_by_id(node_id=find_node_id, properties_types=find_types)\n","                        \n","                        # Если найден узел с похожим содержанием, то\n","                        # создаём отношение: \"Пункт\" -> \"X-старая-сущность\"\n","                        print(f\"Обнаружен похожий текст: `{prepare_text}`; В узле c ID: `{find_node_id}`\")\n","                        graph.add_relationship(source_node_id=paragraph_id_node,\n","                                       target_node_id=find_node_id,\n","                                       rel_type='СОДЕРЖАНИЕ')\n","                        # Сохранение ID узла\n","                        individual_dictionary_nodes_ids[prepare_text] = find_node_id\n","                    \n","                    # Если такого узла не нашлось, то он создаётся\n","                    if individual_dictionary_nodes_ids[prepare_text] == None:\n","                        # Создание отношения: \"Пункт\" -> \"X-новая-сущность\"\n","                        id_node = graph.addition_node(node_text=prepare_text, node_type=prepare_type)\n","                        print(f\"Создан новый узел c ID: `{id_node}`; Текст: `{prepare_text}`\")\n","                        graph.add_relationship(source_node_id=paragraph_id_node,\n","                                               target_node_id=id_node,\n","                                               rel_type='СОДЕРЖАНИЕ')\n","                        # Сохранение ID узла\n","                        individual_dictionary_nodes_ids[prepare_text] = id_node\n","                \n","                # Заполнение в Neo4j распознанных отношений между сущностями\n","                for relationship in graph_document.relationships:\n","                    # Подготовка текста для поиска ID узлов\n","                    prepare_source_node_text = prepare_text_node(relationship.source.id)\n","                    prepare_target_node_text = prepare_text_node(relationship.target.id)\n","\n","                    # Исключение добавления отношений между None сущностями\n","                    if prepare_source_node_text is None or prepare_target_node_text is None:\n","                        continue\n","\n","                    # Поиск ID узлов в словаре\n","                    relationship_source_id = None\n","                    relationship_target_id = None\n","                    if prepare_source_node_text in individual_dictionary_nodes_ids \\\n","                       and prepare_target_node_text in individual_dictionary_nodes_ids:\n","                        relationship_source_id = individual_dictionary_nodes_ids[prepare_source_node_text]\n","                        relationship_target_id = individual_dictionary_nodes_ids[prepare_target_node_text]\n","                    \n","                    print(f\"{relationship_source_id} -> {relationship_target_id}\")\n","\n","                    if relationship_source_id is None \\\n","                       or relationship_target_id is None:\n","                        print(\"Данных об отношении не было найдено:\")\n","                        print(f\"\\tСловарь: {individual_dictionary_nodes_ids}\")\n","                        print(f\"\\tИсточник: `{prepare_source_node_text}`; Цель: `{prepare_target_node_text}`\")\n","\n","                    # Определение названия отношения\n","                    relationship_type = translate_to_rus(relationship.type.replace('_', ' ').replace('-', ' ')).upper().replace(' ', '_').replace('-', '_')\n","\n","                    # Исключение добавления None отношений и отношений на один и тот же узел\n","                    if relationship_source_id is not None \\\n","                       and relationship_target_id is not None \\\n","                       and relationship_source_id != relationship_target_id:\n","\n","                        graph.add_relationship(source_node_id=relationship_source_id,\n","                                               target_node_id=relationship_target_id,\n","                                               rel_type=relationship_type)\n","            \n","            print('-' * 100)\n","            # break  # Выход из цикла при первом результате\n","        except Exception as e:\n","            print(f\"\\nОшибка при использовании модели {model_id}: {e}\\n\")\n","            continue\n","    \n","    if all_models_failed:\n","        print(\"\\nВсе модели выдали плохой результат.\")\n","        if not stop:\n","            print(\"\\n\\tПерезапуск анализа чанка, но без фильтра...\")\n","            result = filling_in_graph(document, paragraph_id_node, graph, models, filter=False)\n","        if stop:\n","            print(\"\\nМодели продолжают выдавать плохой результат. Пропуск чанка.\")\n","            print('-' * 100)\n","            return False\n","        return result\n","    return True"]},{"cell_type":"markdown","id":"ecee8229","metadata":{},"source":["### 2.2.3 - Запуск анализа выбранных документов"]},{"cell_type":"code","execution_count":null,"id":"e0ab8f50","metadata":{},"outputs":[],"source":["# Инициализируем граф\n","graph = InteractionNeo4jGraph()\n","\n","\n","# Основной цикл обработки файлов\n","for filename in pdf_texts:\n","    document_info = pdf_texts[filename]['doc_info']\n","\n","    # Пропуск не идентифицированных документов\n","    if document_info is None:\n","        continue\n","    \n","    # Идентификация документа\n","    prepare_org_name = prepare_text_node(document_info['organization'])\n","    prepare_doc_number = prepare_text_node(document_info['document_number'])\n","    doc_type = document_info['document_type']\n","    doc_date = document_info['date']\n","\n","    # Создание отношения: \"Организация\" -> \"Документ\"\n","    org_id_node = graph.addition_node(node_text=prepare_org_name, node_type=prepare_type_node(\"Организация\"))\n","    doc_info_id_node = graph.addition_node(node_text=prepare_doc_number, node_type=prepare_type_node(\"НомерДокумента\"), properties={\"тип\": doc_type,\"дата\": doc_date})\n","    graph.add_relationship(source_node_id=org_id_node,\n","                           target_node_id=doc_info_id_node,\n","                           rel_type=\"ИЗДАННО\")\n","    \n","    # Чтение информации о созданных узлах документа из лог-файла\n","    log_file_data = processed_log.get(filename, None)\n","\n","    # Привязывание распознаваемых сущностей к заголовкам и пунктам\n","    for titles_and_subpoints in pdf_texts[filename]['titles_and_subpoints']:\n","\n","        # Если документ имеет хотя бы один заголовок, то создаётся отношение:\n","        #   \"Документ\" -> \"Заголовок\"\n","        title_id_node = None\n","        log_title_data = None\n","        prepare_title = prepare_text_node(titles_and_subpoints)\n","        if log_file_data:\n","            # Поиск в лог-файле ID узла \"Заголовок\"\n","            log_title_data = log_file_data.get(prepare_title, None)\n","        if prepare_title != 'Без заголовка':\n","            if log_title_data:\n","                # Если в лог-файле обнаружен ID узла \"Заголовок\", то используем его\n","                title_id_node = log_title_data[0].get(\"title_id_node\", None)\n","            if title_id_node:\n","                print(f\"Узел `Заголовок`: `{prepare_title}` уже существует; ID: {title_id_node}\")\n","            # Иначе узел \"Заголовок\" создаётся\n","            else:\n","                title_id_node = graph.create_node(node_text=prepare_title, node_type=prepare_type_node('Заголовок'))\n","                graph.add_relationship(source_node_id=doc_info_id_node,\n","                                       target_node_id=title_id_node,\n","                                       rel_type='ЗАГОЛОВОК')\n","\n","        # Обработка (под)пунктов\n","        for text in pdf_texts[filename]['titles_and_subpoints'][titles_and_subpoints]:\n","            \n","            # Если документ имеет хотя бы один заголовок, то создаётся отношение:\n","            #   \"Заголовок\" -> \"Пункт\"\n","            prepare_paragraph = prepare_text_node(f\"{'.'.join(get_item_numbers(text))}.\")\n","            paragraph_id_node = None\n","            if log_file_data and log_title_data:\n","                # Поиск в лог-файле ID узла \"Пункт\"\n","                paragraph_data = log_title_data[0].get(prepare_paragraph, {})\n","            if titles_and_subpoints != 'Без заголовка':\n","                if log_file_data and paragraph_data:\n","                    # Если в лог-файле обнаружен ID узла \"Пункт\", то используем его\n","                    paragraph_id_node = paragraph_data.get(\"paragraph_id_node\", None)\n","                # Если в лог-файле обнаружен ID узла \"Пункт\", то используем его\n","                if paragraph_id_node:\n","                    print(f\"Узел `Пункт`: `{prepare_paragraph}` уже существует; ID: {paragraph_id_node}\")\n","                    # Если в лог-файле обнаружено, что обработка не закончена, то продолжаем\n","                    status_paragraph = paragraph_data.get(\"status\", None)\n","                    if status_paragraph == 'completed':\n","                        continue\n","                # Иначе узел \"Пункт\" создаётся\n","                else:\n","                    paragraph_id_node = graph.create_node(node_text=prepare_paragraph, node_type=prepare_type_node('Пункт'))\n","                    graph.add_relationship(source_node_id=title_id_node,\n","                                           target_node_id=paragraph_id_node,\n","                                           rel_type='СОДЕРЖАНИЕ')\n","            \n","            # Иначе узел \"Заголовок\" в цепочке пропускается. Остаётся:\n","            #   \"Документ\" -> \"Пункт\"\n","            else:\n","                if log_file_data:\n","                    # Поиск в лог-файле ID узла \"Пункт\"\n","                    paragraph_id_node = paragraph_data.get(\"paragraph_id_node\", None)\n","                # Если в лог-файле обнаружен ID узла \"Пункт\", то используем его\n","                if log_file_data and paragraph_id_node:\n","                    print(f\"Узел `Пункт`: `{prepare_paragraph}` уже существует; ID: {paragraph_id_node}\")\n","                    # Если в лог-файле обнаружено, что обработка не закончена, то продолжаем\n","                    status_paragraph = paragraph_data.get(\"status\", None)\n","                    if status_paragraph == 'completed':\n","                        continue\n","                # Иначе узел \"Пункт\" создаётся\n","                else:\n","                    paragraph_id_node = graph.create_node(node_text=prepare_paragraph, node_type=prepare_type_node('Пункт'))\n","                    graph.add_relationship(source_node_id=doc_info_id_node,\n","                                           target_node_id=paragraph_id_node,\n","                                           rel_type='СОДЕРЖАНИЕ')\n","\n","            # Отчет о том что узлы уже созданы в графе, нужно найти сущности и отношения\n","            update_log(log_file, filename, prepare_title, title_id_node, prepare_paragraph, paragraph_id_node, processed_log, 'in progress')\n","\n","            # Удаление их текста номера пункта\n","            text = text[len(prepare_paragraph):]\n","\n","            # Суммаризация текста до заданной длины чанка\n","            if len(text) > chunk_size:\n","                text = summary_text(text, chunk_size, summary_tokenizer, summary_model)\n","                # print(text)\n","\n","            # Принятие мер, если суммаризированный текст слишком велик\n","            if len(text) > chunk_size:\n","                # Разделение текста на предложения\n","                text = sent_tokenize(text)\n","                # Если текст это одно предложение, то оно делится по середине, согласно пунктуации\n","                if len(text) == 1:\n","                    text = split_text_by_punctuation(text[0], chunk_size)\n","\n","            if isinstance(text, str):\n","                if filling_in_graph([Document(page_content=text)], paragraph_id_node, graph, models, True):\n","                    update_log(log_file, filename, prepare_title, title_id_node, prepare_paragraph, paragraph_id_node, processed_log, 'completed')\n","            elif isinstance(text, list):\n","                for text_small in text:\n","                    # Последняя попытка суммаризации большого предложения\n","                    if len(text_small) > chunk_size:\n","                        text_small = summary_text(text_small, chunk_size, summary_tokenizer, summary_model)\n","                    if filling_in_graph([Document(page_content=text_small)], paragraph_id_node, graph, models, True):\n","                        update_log(log_file, filename, prepare_title, title_id_node, prepare_paragraph, paragraph_id_node, processed_log, 'completed')\n","    \n","    break # Обработать один документ\n","\n","# Закрываем соединение с графом\n","graph.close()"]},{"cell_type":"markdown","id":"4b1cc8b1","metadata":{},"source":["# 3 - Использование RAG графа"]},{"cell_type":"markdown","id":"e44665f7","metadata":{},"source":["## 3.1 - Инициализация графа"]},{"cell_type":"code","execution_count":null,"id":"b394c9c8","metadata":{},"outputs":[],"source":["from langchain.graphs import Neo4jGraph\n","graph = Neo4jGraph(\n","    url=os.getenv(\"NEO4J_URI\"), \n","    username=os.getenv(\"NEO4J_USERNAME\"), \n","    password=os.getenv(\"NEO4J_PASSWORD\")\n",")"]},{"cell_type":"markdown","id":"daca14f1","metadata":{},"source":["## 3.2 - Генерация запроса Cypher и получение ответа от LLM"]},{"cell_type":"code","execution_count":null,"id":"3f13a690","metadata":{},"outputs":[],"source":["# Задаём вопрос\n","question = \"\"\"\n","Чем является компьютерная атака?\n","\"\"\".strip()"]},{"cell_type":"code","execution_count":null,"id":"64411539","metadata":{},"outputs":[],"source":["from langchain.chains import GraphCypherQAChain"]},{"cell_type":"markdown","id":"0d3622bd","metadata":{},"source":["### 3.2.1 - Использование GigaChat"]},{"cell_type":"code","execution_count":null,"id":"a026960d","metadata":{},"outputs":[],"source":["from langchain.chat_models.gigachat import GigaChat"]},{"cell_type":"code","execution_count":null,"id":"46287785","metadata":{},"outputs":[],"source":["gigachat_api_key = getpass(prompt='Введите API ключ (Авторизационные данные) от GigaChat')"]},{"cell_type":"code","execution_count":null,"id":"fb8c23f3","metadata":{},"outputs":[],"source":["model = GigaChat(credentials=gigachat_api_key, verify_ssl_certs=False, temperature=0.01)\n","\n","cypher_chain = GraphCypherQAChain.from_llm(\n","    cypher_llm=model,\n","    qa_llm=model,\n","    graph=graph, \n","    verbose=True,\n","    return_intermediate_steps=True\n",")\n","\n","answer = cypher_chain.run(question)\n","print(answer)"]},{"cell_type":"markdown","id":"d96a74c2","metadata":{},"source":["### 3.2.2 - Использование LLM с HuhuggingFace"]},{"cell_type":"code","execution_count":null,"id":"f453f504","metadata":{},"outputs":[],"source":["for model in models:\n","    try:\n","        cypher_chain = GraphCypherQAChain.from_llm(\n","            cypher_llm=initialize_endpoint(model),\n","            qa_llm=initialize_endpoint(model),\n","            graph=graph, \n","            verbose=True,\n","            return_intermediate_steps=True\n","        )\n","        # Задаём вопрос\n","        answer = cypher_chain.run(question)\n","        print(answer)\n","    except Exception as e:\n","        print(f\"\\nОшибка при использовании модели {model}: {e}\\n\")\n","        continue"]},{"cell_type":"markdown","id":"f4b6e6c7","metadata":{},"source":["## 3.2 - Использование векторного индекса"]},{"cell_type":"code","execution_count":null,"id":"5ab866ac","metadata":{},"outputs":[],"source":["from langchain.vectorstores.neo4j_vector import Neo4jVector\n","from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n","from langchain.chains import RetrievalQA"]},{"cell_type":"code","execution_count":null,"id":"09482cf5","metadata":{},"outputs":[],"source":["vector_index = Neo4jVector.from_existing_graph(\n","    HuggingFaceEmbeddings(),\n","    url=os.getenv(\"NEO4J_URI\"),\n","    username=os.getenv(\"NEO4J_USERNAME\"),\n","    password=os.getenv(\"NEO4J_PASSWORD\"),\n","    index_name='tasks',\n","    node_label=\"Task\",\n","    text_node_properties=['text', 'типы'],\n","    embedding_node_property='embedding',\n",")"]},{"cell_type":"markdown","id":"33b17795","metadata":{},"source":["# Поле экспериментальных способов получить внятный ответ от LLM"]},{"cell_type":"markdown","id":"ac59d04c","metadata":{},"source":["## Попытка сгенерировать запрос для трассировки от сущности до ключевого слова"]},{"cell_type":"code","execution_count":null,"id":"79f97e87","metadata":{},"outputs":[],"source":["# Используем регулярные выражения для извлечения JSON-структуры\n","def extract_json(text):\n","    # Регулярное выражение для поиска JSON-объекта\n","    json_pattern = re.compile(r'\\{.*\\}', re.DOTALL)\n","    match = json_pattern.search(text)\n","    if match:\n","        return match.group(0)\n","    return None"]},{"cell_type":"code","execution_count":null,"id":"9328b941","metadata":{},"outputs":[],"source":["from langchain.chains import LLMChain\n","from langchain_core.prompts import PromptTemplate\n","import json\n","from json_repair import repair_json"]},{"cell_type":"code","execution_count":null,"id":"2aec3336","metadata":{},"outputs":[],"source":["# Шаблон для запроса\n","prompt_template = \"\"\"User Instructions:\n","You will provide a query related to regulatory documents on information security. The LLM will extract relevant entities and keywords from your query and output a JSON containing these entities/keywords. The JSON can then be used to form a search query in a Neo4j graph database.\n","\n","LLM Instructions:\n","You are provided with a user query related to regulatory documents on information security. Your task is to extract relevant entities and keywords from the query and output a JSON containing these entities/keywords.\n","Ensure both:\n","- entities\n","- keywords\n","are present in the JSON, even if they are empty arrays.\n","\n","Additionally, make sure to include at least one entity and one keyword to trace the graph for finding an answer.\n","\n","User Query:\n","{user_query}\n","\n","LLM Response:\"\"\"\n","\n","# Шаблон для вопроса и ответа\n","prompt = PromptTemplate(template=prompt_template, input_variables=[\"user_query\"])"]},{"cell_type":"code","execution_count":null,"id":"796f72f1","metadata":{},"outputs":[],"source":["all_entities = []\n","all_keywords = []\n","\n","def split_and_trim(text):\n","    \"\"\"\n","    Функция для разбивки текста на части и удаления половины символов у каждой части.\n","    \"\"\"\n","    parts = text.split()\n","    parts = [part[:len(translate_to_rus(part))//2] for part in parts]\n","    return parts\n","\n","for model in models:\n","    # Настройка модели\n","    llm = initialize_endpoint(model)\n","    llm_chain = LLMChain(prompt=prompt, llm=llm)\n","    generated_text = llm_chain.invoke({\"user_query\": question})['text'].strip()\n","    print(generated_text)\n","\n","    # Извлекаем JSON-объект из текста\n","    try:\n","        # Найти JSON в тексте\n","        json_match = re.search(r'\\{.*?\\}', generated_text, re.DOTALL)\n","        if json_match:\n","            json_text = json_match.group(0)\n","            extracted_json = json.loads(json_text)\n","            entities = extracted_json.get(\"entities\", [])\n","            keywords = extracted_json.get(\"keywords\", [])\n","\n","            if entities:\n","                for entity in entities:\n","                    entity = lemmatize_word(entity.lower())\n","                    entity_parts = split_and_trim(entity)\n","                    for entity_part in entity_parts:\n","                        if entity_part not in all_entities:\n","                            all_entities.append(entity_part)\n","\n","            if keywords:\n","                for keyword in keywords:\n","                    keyword = lemmatize_word(keyword.lower())\n","                    keyword_parts = split_and_trim(keyword)\n","                    for keyword_part in keyword_parts:\n","                        if keyword_part not in all_keywords:\n","                            all_keywords.append(keyword_part)\n","    except Exception as e:\n","        print(f\"\\nОшибка при использовании модели {model}: {e}\\n\")\n","        continue\n","\n","# Удаление дубликатов\n","all_entities = list(set(all_entities))\n","all_keywords = list(set(all_keywords))\n","\n","# Убираем дубликаты условий\n","unique_conditions = list(set(all_keywords) - set(all_entities))\n","\n","# Формируем Cypher-запрос\n","entity_conditions = [f\"toLower(start.text) CONTAINS toLower('{entity}')\" for entity in all_entities]\n","keyword_conditions = [f\"toLower(end.text) CONTAINS toLower('{keyword}')\" for keyword in unique_conditions]\n","\n","entity_conditions_str = \" OR \".join(entity_conditions)\n","keyword_conditions_str = \" OR \".join(keyword_conditions)\n","\n","cypher_query = f\"\"\"\n","MATCH (start), (end)\n","WHERE ({entity_conditions_str}) AND ({keyword_conditions_str})\n","MATCH path = (start)-[*]->(end)\n","RETURN path\n","\"\"\"\n","\n","print(cypher_query)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":5}
